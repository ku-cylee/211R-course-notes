\section{Expectation}

\subsection{Expectation}

\subsubsection*{Expectation}
\begin{itemize}
    \item \textbf{Def.} Expected value(= expectation = mean) of a discrete r.v. $X$ is
    \begin{equation}
        E(X)=\sum_{i=1}^\infty x_iP(X=x_i)=\sum_{x}xp_X(x)
    \end{equation}
    \item $E(X)$ depends only on the distribution of $X$
    \item Linearity of expectation
    \begin{itemize}
        \item For all r.v.s $X$, $Y$, $E(X+Y)=E(X)+E(Y)$
        \item For all r.v.s $X$ and $c\in\mathbb{R}$, $E(cX)=cE(X)$
    \end{itemize}
\end{itemize}

\subsubsection*{Indicator Random Variable}
\begin{itemize}
    \item Indicator random variable: R.v. is $1$ if event $A$ occurs(success) and $0$ otherwise (failure)
    \begin{equation}
        I_A(s)=\begin{cases}
            1 & s\in A\\
            0 & s\notin A
        \end{cases}
    \end{equation}
    \item Properties of indicator r.v.
    \begin{itemize}
        \item $\forall~k\in\mathbb{N}$, $(I_A)^k=I_A$; $I_{A^c}=1-I_A$; $I_{A\cap B}=I_AI_B$; $I_{A\cup B}=I_A+I_B-I_AI_B$
    \end{itemize}
    \item Indicator r.v. is a fundamental bridge between probability and expectation
    \begin{equation}
        P(A)=E(I_A)
    \end{equation}
\end{itemize}

\subsubsection*{Law of the Unconscious Statistician (LOTUS)}
\begin{itemize}
    \item $X$ is a discrete r.v. and $g:\mathbb{R}\to\mathbb{R}$, then for $Y=g(X)$,
    \begin{equation}
        E(Y)=E(g(X))=\sum_xg(x)P(X=x)
    \end{equation}
    \item Be careful: $E(g(X))\neq g(E(X))$ (St. Petersburg Paradox)
\end{itemize}

\subsubsection*{Variance}
\begin{itemize}
    \item Variance of an r.v. $X$
    \begin{equation}
        \text{Var}(X):=E\Bigr[(X-E(X))^2\Bigr]
    \end{equation}
    \item Standard deviation of an r.v. $X$ is $\text{SD}(X):=\sqrt{\text{Var}(X)}$
    \item For any r.v. $X$,
    \begin{equation}
        \text{Var}(X)=E\left(X^2\right)-\left(E(X)\right)^2
    \end{equation}
    \item Properties
    \begin{itemize}
        \item $\text{Var}(X+c)=\text{Var}(X)$
        \item $\text{Var}(cX)=c^2\text{Var}(X)$
        \item $X\indep Y\implies \text{Var}(X+Y)=\text{Var}(X)+\text{Var}(Y)$
        \item $\text{Var}(X)\geq 0$
        \item Variance is not linear
    \end{itemize}
\end{itemize}

\subsubsection*{Binomial Distribution}
\begin{itemize}
    \item For $X\sim\text{Bin}(n,p)$ and $q=1-p$,
    \item $E(X)=np$
    \item $\text{Var}(X)=np(1-p)=npq$
\end{itemize}

\subsubsection*{Geometric Distribution}
\begin{equation}
    X\sim\text{Geom}(p)\implies P(X=k)=(1-p)^kp~~(k=0,1,\cdots)
\end{equation}
\begin{itemize}
    \item \# of \textit{failures} before the first success in a sequence of i.i.d. Bernoulli trials with success prob. $p$
    \item First success distribution
    \begin{itemize}
        \item \# of \textit{trials} before the first success in a sequence of i.i.d. Bernoulli trials with success prob. $p$
        \item $Y\sim\text{FS}(p)\implies P(Y=k)=(1-p)^{k-1}p~~(k=1,2,\cdots)$, $Y=X+1$
    \end{itemize}
    \item Expectation
    \begin{equation}
        E(X)=\frac{1-p}{p},~~E(Y)=\frac{1}{p}
    \end{equation}
    \item Variance
    \begin{equation}
        \text{Var}(X)=\text{Var}(Y)=\frac{1-p}{p^2}
    \end{equation}
\end{itemize}

\subsubsection*{Poisson Distribution}
\begin{equation}
    P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}~~(k=0,1,2,\cdots)
\end{equation}
\begin{itemize}
    \item $X$: \# of events occurs during the time interval $\lambda$
    \item Expectation and Variance
    \begin{equation}
        E(X)=\text{Var}(X)=\lambda
    \end{equation}
    \item Poisson dist. is a \textit{continuous} version of Binomial dist.
    \begin{itemize}
        \item Suppose $X\sim\text{Bin}(n,p)$ and $\lambda=np$
        \item Let $n\to\infty$ and $p\to 0$ with $\lambda$ fixed, then $p_X$ converges to $\text{Pois}(\lambda)$ PMF
        \item Parameter $\lambda>0$: Interpreted as a \textit{rate} of occurrence of \textit{rare} events during unit time
    \end{itemize}
    \item $X\sim\text{Pois}(\lambda_1)$, $Y\sim\text{Pois}(\lambda_2)$, $X\indep Y$
    \begin{itemize}
        \item $X+Y\sim\text{Pois}(\lambda_1+\lambda_2)$
        \item $X\mid X+Y=n\sim\text{Bin}\left(n,\frac{\lambda_1}{\lambda_1+\lambda_2}\right)$
    \end{itemize}
\end{itemize}
