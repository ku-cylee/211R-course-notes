\section{Neural Networks}

\subsection{Artificial Neural Networks}

\subsubsection*{Motivation}
\begin{itemize}
    \item Pros and cons of linear classifier
    \begin{itemize}
        \item Pros: Simple $\Rightarrow$ Very expressive
        \item Cons: Not very powerful (e.g. XOR problem)
    \end{itemize}
    \item Human's thinking process is hierachial (low level $\rightarrow$ high level)
    \begin{itemize}
        \item e.g. Book reading: letters $\rightarrow$ words $\rightarrow$ sentences $\rightarrow$ story
    \end{itemize}
    \item Insert non-linear models between linear models $\Rightarrow$ \textbf{Deep Learning}
    \item Non-linear function $f$ is called ``activation function''
    \begin{itemize}
        \item $f$ is elementwise, i.e. $f\left((x_1,x_2,\cdots,x_n)\right)=(f(x_1),f(x_2),\cdots,f(x_n))$
    \end{itemize}
\end{itemize}
\begin{figures}
    \subfig{Nonlinear models}{nonlinear-models.png}{.3808}
    \subfig{Activation function}{activation-function.png}{.4192}
\end{figures}

\subsubsection*{Multilayer Neural Network}
\begin{itemize}
    \item $n$-layer neural network, or $(n-1)$-hidden-layer neural network
    \item 1 input layer $\rightarrow$ $(n-1)$ hidden layer $\rightarrow$ 1 output layer
    \item Scores on hidden layer are called ``intermediate output''
    \item Every hidden \& output layer has non-linear function $f$
    \begin{itemize}
        \item If not, it can be merged to linear model
    \end{itemize}
    \item Fully connected(FC) layers: Every layers are connected each other
\end{itemize}
\begin{figures}
    \fig{multilayer-neural-network.png}{.45}
\end{figures}
\clearpage

\subsubsection*{Computing Neural Network}
\begin{itemize}
    \item Consider $k$-th(input) layer has $m$ classes $\Rightarrow$ Scores $x=x_1,x_2,\cdots,x_m$
    \item Consider $(k+1)$-th(output) layer has $n$ classes $\Rightarrow$ Scores $y=y_1,y_2,\cdots,y_n$
    \item Let $w_{ij}$ be the parameter from $i$-th input to $j$-th output, $w_j=(w_{1j},w_{2j},\cdots,w_{mj})$
    \item Let $b_j$ be the bias from input layer to $j$-th output
    \item Let $f$ be the activation function on output layer
    \item Then, value of $j$-th output is
    \begin{equation}
        y_j=f(w_{1j}x_1+w_{2j}x_2+\cdots+w_{ij}x_i+b_j)=f(w_j^Tx+b_j)
    \end{equation}
    \item Let $W=\left(w_1^T,w_2^T,\cdots,w_n^T\right)$ and $b=(b_1,b_2,\cdots,b_n)$, then
    \begin{equation}
        y=f(Wx+b),\text{ i.e. }\begin{mtx}{c}
            y_1 \\ y_2 \\ \vdots \\ y_n
        \end{mtx}=f\left(\begin{mtx}{cccc}
            w_{11} & w_{21} & \cdots & w_{m1}\\
            w_{12} & w_{22} & \cdots & w_{m2}\\
            \vdots & \vdots & \ddots & \vdots\\
            w_{1n} & w_{2n} & \cdots & w_{mn}
        \end{mtx}\begin{mtx}{c}
            x_1 \\ x_2 \\ \vdots \\ x_m
        \end{mtx}+\begin{mtx}{c}
            b_1 \\ b_2 \\ \vdots \\ b_n
        \end{mtx}\right)
    \end{equation}
    \item For $k$-layer nerual network, $y=f_k(W_kf_{k-1}(W_{k-1}\cdots f_1(W_1x)))$
\end{itemize}
\begin{figures}
    \fig{neural-network-calculation.png}{.25}
\end{figures}

\subsubsection*{Activation Functions}
\begin{itemize}
    \item ReLU (Rectified Linear Unit)
    \begin{equation}
        f(x)=\max\{0,x\}
    \end{equation}
    \begin{itemize}
        \item Filters out $x<0$ $\implies$ rectifies information
        \item Simple calculation, similar to linear $\implies$ Nice optimization and expression
        \item Good choice for many problems
    \end{itemize}
    \item Sigmoid ($\sigma(x)=\frac{1}{1+e^{-x}}$), Hyper-tangent ($f(x)=\tanh(x)$)
    \item Leaky ReLU ($f(x)=\max\{0.01x,x\}$), ELU ($f(x)=x~(x\geq 0),~\alpha(e^x-1)~(x<0)$)
\end{itemize}
\begin{figures}
    \subfig{ReLU}{actfunc-relu.jpg}{.18}
    \subfig{Sigmoid}{actfunc-sigmoid.jpg}{.18}
    \subfig{tanh}{actfunc-tanh.jpg}{.18}
    \subfig{Leaky ReLU}{actfunc-leaky-relu.jpg}{.18}
    \subfig{ELU}{actfunc-elu.jpg}{.18}
\end{figures}

\subsection{Optimizing Loss Functions}

\subsubsection*{Optimization}
\begin{itemize}
    \item Optimization: to minimize or maximize a function $f:\mathbb{R}^n\to\mathbb{R}$
    \item $f(x)$ is minimum at $x=x^\ast$ $\Rightarrow$ $x^\ast$: optimal point, $f(x^\ast)$: optimal value
    \begin{itemize}
        \item $x=x^\ast$ is (global) optimal: $\forall x\in\mathcal{D}(f)$, $f(x^\ast)\leq f(x)$
        \item $x=x^\ast$ is local optimal: $\exists~R>0$ s.t. $x^\ast$ is optimal point for $f$ at $\Vert x-x^\ast\Vert\leq R$
    \end{itemize}
    \item Optimization methods
    \begin{itemize}
        \item If $\exists~f^\prime$, all optimal points $x=x^\ast$ satisfies $\nabla f(x^\ast)=0$
        \item Produce a sequence of points $x^{(k)}$ for $k=1,2,\cdots$ such that $f\left(x^{(k)}\right)\to f\left(x^\ast\right)$
    \end{itemize}
\end{itemize}

\subsubsection*{Gradient}
\begin{itemize}
    \item Gradient is a vector of partial derivatives of function $f:\mathbb{R}^n\rightarrow\mathbb{R}$
    \begin{equation}
        \nabla f(x)=\left(\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2},\cdots,\frac{\partial f(x)}{\partial x_n}\right)
    \end{equation}
    \item Taylor expansion: $f(x + \Delta x) \approx f(x) + \nabla f(x)^T \Delta x$
    \item Example
    \begin{itemize}
        \item $f(x)=c^Tx$ $\Rightarrow$ $\nabla f(x)=c$
        \item $f(x)=x^TQx$ $\Rightarrow$ $\nabla f(x)=2Qx$
    \end{itemize}
    \item Gradient vector $\nabla f$ points in the direction of greatest increase/decrease of $f$
    \begin{itemize}
        \item Directional derivative $\nabla f(x)^T\Delta x$: Rate of change to direction $\Delta x$
        \item For any $\Vert\Delta x\Vert=1$, $f(x)$ increases the most if $\Delta x=\frac{\nabla f(x)}{\Vert\nabla f(x)\Vert}$
        \item For any $\Vert\Delta x\Vert=1$, $f(x)$ decreases the most if $\Delta x=-\frac{\nabla f(x)}{\Vert\nabla f(x)\Vert}$
    \end{itemize}
\end{itemize}

\subsubsection*{Gradient Descent (GD) Method}
\begin{itemize}
    \item Algorithm: General descent method with $\Delta x = -\nabla f(x)$
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, tolerance $\epsilon>0$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Determine a descent direction $\Delta x=-\nabla f(x)$
            \item \textit{Line search} Choose a step size $\alpha>0$
            \item \textit{Update} $x:=x+\alpha\Delta x$
        \end{enumerate}
        \item[until] stopping criterion is satisfied; usually $\Vert\nabla f(x)\Vert_2\leq\epsilon$
    \end{description}
    \item Pros of GD
    \begin{itemize}
        \item Convergence result: $f(x^{(k)})-p^\ast\leq c^k\left(f(x^{(0)})-p^\ast\right)$, $c\in(0,1)$
        \item Convergence is guaranteed and mostly very fast ($\because$ exponential)
        \item Most cases, simple and efficient
    \end{itemize}
    \item Cons of GD
    \begin{itemize}
        \item Myopic algorithm: Can be very slow for some problems ($c\to 1$)
        \item GD may find a local optimal, but not necessarily global optimal
    \end{itemize}
\end{itemize}

\subsubsection*{GD Algorithm for Optimizing Loss Functions}
\begin{itemize}
    \item Objective of deep learning is to optimize loss function $L(W)$
    \item By applying GD algorithm, repeat
    \begin{equation}
        W\leftarrow W-\alpha\nabla L(W)
    \end{equation}
    \item GD may fail to find global optimal
    \begin{itemize}
        \item But, local optimal values of neural networks are similar
        \item Not a big problem at neural networks
    \end{itemize}
    \item Directly computing $\nabla L(W)$ by differentiation is complicated! $\Rightarrow$ Backpropagation
\end{itemize}

\subsection{Backpropagation}

\subsubsection*{Jacobian}
\begin{itemize}
    \item Partial derivative of multidimensional mapping
    \item $x\in\mathbb{R}^m$, $y\in\mathbb{R}^n$ $\implies$ $\frac{\partial y}{\partial x}:m\times n$ matrix
    \begin{equation}
        \frac{\partial y}{\partial x}=\begin{mtx}{cccc}
            \nabla y_1(x)^T \\
            \nabla y_2(x)^T \\
            \vdots \\
            \nabla y_m(x)^T
        \end{mtx}=\begin{mtx}{cccc}
            \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n}\\
            \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n}\\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n}
        \end{mtx}
    \end{equation}
    \item $X\in\mathbb{R}^{m\times n}$, $y\in\mathbb{R}$ $\implies$ $\frac{\partial y}{\partial X}:m\times n$ matrix
    \begin{equation}
        \frac{\partial y}{\partial X}=\begin{mtx}{cccc}
            \frac{\partial y}{\partial x_{11}} & \frac{\partial y}{\partial x_{12}} & \cdots & \frac{\partial y}{\partial x_{1n}}\\
            \frac{\partial y}{\partial x_{21}} & \frac{\partial y}{\partial x_{22}} & \cdots & \frac{\partial y}{\partial x_{2n}}\\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial y}{\partial x_{m1}} & \frac{\partial y}{\partial x_{m2}} & \cdots & \frac{\partial y}{\partial x_{mn}}
        \end{mtx}
    \end{equation}
    \item $X\in\mathbb{R}^{m\times n}$, $y\in\mathbb{R}^k$ $\implies$ $\frac{\partial y}{\partial X}:k\times m\times n$ matrix(=tensor)
    \begin{equation}
        \frac{\partial y}{\partial X}=\left(\begin{mtx}{ccc}
            \frac{\partial y_1}{\partial x_{11}} & \cdots & \frac{\partial y_1}{\partial x_{1n}}\\
            \vdots & \ddots & \vdots \\
            \frac{\partial y_1}{\partial x_{m1}} & \cdots & \frac{\partial y_1}{\partial x_{mn}}
        \end{mtx},\begin{mtx}{ccc}
            \frac{\partial y_2}{\partial x_{11}} & \cdots & \frac{\partial y_2}{\partial x_{1n}}\\
            \vdots & \ddots & \vdots \\
            \frac{\partial y_2}{\partial x_{m1}} & \cdots & \frac{\partial y_2}{\partial x_{mn}}
        \end{mtx},\cdots,\begin{mtx}{ccc}
            \frac{\partial y_k}{\partial x_{11}} & \cdots & \frac{\partial y_k}{\partial x_{1n}}\\
            \vdots & \ddots & \vdots \\
            \frac{\partial y_k}{\partial x_{m1}} & \cdots & \frac{\partial y_k}{\partial x_{mn}}
        \end{mtx}\right)
    \end{equation}
    \item Jacobian is a \textit{transpose} of gradient
    \item Chain rule: For $y=h(g(f(x)))$,
    \begin{equation}
        \frac{\partial y}{\partial x}=\frac{\partial y}{\partial g}\frac{\partial g}{\partial f}\frac{\partial f}{\partial x}
    \end{equation}
    \begin{itemize}
        \item Sequence of multiplication is important
    \end{itemize}
\end{itemize}
\clearpage

\subsubsection*{Backpropagation}
\begin{itemize}
    \item Computational Graph: Computation relations expressed in graph
    \begin{figures}
        \fig{computational-graph.png}{.7}
    \end{figures}
    \begin{itemize}
        \item Computational graph of $f(w,x)=\sigma(w_0x_0+w_1x_1+w_2)$
        \item Each node represents function (e.g. add., mult., exponential, sigmoid, etc.)
    \end{itemize}
    \item Forward computation: Compute loss along graph from given input values
    \item Backpropagation: Use chain rule and gradient from next node to compute previous nodes' gradients
    \begin{itemize}
        \item Suppose a node: input $x$ and $y$, output $z$, function $f$, i.e. $z=f(x,y)$
        \item Let $L$ be a loss of graph
        \item Upstream gradient(s): $\frac{\partial L}{\partial z}$; given from next node
        \begin{itemize}
            \item Upstream gradient of last node is always $1$
        \end{itemize}
        \item Local gradient(s): $\frac{\partial z}{\partial x}$, $\frac{\partial z}{\partial y}$; can be computed from function $f$ and input/output values of the node
        \item Use chain rule to compute $\frac{\partial L}{\partial x}$, $\frac{\partial L}{\partial y}$
        \begin{equation}
            \frac{\partial L}{\partial x}=\frac{\partial L}{\partial z}\frac{\partial z}{\partial x},~\frac{\partial L}{\partial y}=\frac{\partial L}{\partial z}\frac{\partial z}{\partial y}
        \end{equation}
        \item Computed gradients are \textit{propagated} to previous layers $\implies$ \textbf{Backpropagation}
    \end{itemize}
    \item Patterns in backward flow
    \begin{itemize}
        \item add gate is gradient ``distributor'': propagates to previous as it is
        \item max gate is gradient ``router'': only propagates to max value node
        \item mult gate is gradient ``switcher'': inputs switch, then propagates with upstream gradient
    \end{itemize}
    \item Nodes may have branches, i.e. multiple branches
    \begin{equation}
        y=f(x),~z=g(x)\implies\frac{\partial L}{\partial x}=\frac{\partial L}{\partial y}\frac{\partial y}{\partial x}+\frac{\partial L}{\partial z}\frac{\partial z}{\partial x}
    \end{equation}
    \item Optimizing computation
    \begin{itemize}
        \item Matrix multiplication is \textit{expensive} for huge matrices
        \item Consider $y=f(x)=\max\{x,0\}$, then
        \begin{equation}
            \frac{\partial L}{\partial x_{ij}}=\begin{cases}
                \frac{\partial L}{\partial y_{ij}} & (x_{ij}>0) \\
                0 & (x_{ij}\leq 0)
            \end{cases}
        \end{equation}
        \item Sometimes, element-wise computation is cheaper then matrix multiplication
    \end{itemize}
\end{itemize}

\subsection{Convolutional Neural Networks}

\subsubsection*{Limitations of ANN with FC layers}
\begin{itemize}
    \item On image recognition, spatial information(local structures) are important
    \item In FC layers, spatial information is lost!
    \item By subsampling input images, get spatial information $\implies$ \textit{convolution}
\end{itemize}
\begin{figures}
    \fig{cnn.png}{.5}
\end{figures}

\subsubsection*{Convolutional Layer}
\begin{itemize}
    \item Suppose input data(image) $x$ is
    \begin{equation}
        x:~D\times W_i\times H_i
    \end{equation}
    \item Suppose \# of channels $C\in\mathbb{N}$, stride $S\in\mathbb{N}$
    \item Suppose $c$-th filter and $c$-th bias $W_c$ and $b_c\in\mathbb{R}$
    \begin{equation}
        W_c:~D\times W_f\times H_f~(W_f\leq W_i,~H_f\leq H_i)
    \end{equation}
    \item Shape of output data
    \begin{equation}
        y:~C\times\left(W_o=\frac{W_i-W_f}{S}+1\right)\times\left(H_o=\frac{H_i-H_f}{S}+1\right)
    \end{equation}
    \item Computing output data
    \begin{equation}
        y_{w_o,h_o,c}=\sum_{h_f=1}^{H_f}\sum_{w_f=1}^{W_f}\sum_{d=1}^DW_{c,d,w_f,h_f}\cdot x_{d,(w_0-1)S+w_f,(h_0-1)S+h_f}+b_c
    \end{equation}
    \begin{enumerate}
        \item Reshape $W_c$ to a vector $\tilde{w}$
        \item Extract a $D\times W_f\times H_f\times$ matrix from $W_c$ to a vector $\tilde{x}$
        \item Compute inner product: $y_{w_o,h_o,c}=\tilde{w}^T\tilde{x}+b_c$
        \item Slide the filter by stride $S$, and repeat the process
        \item Repeat the process for every channel
    \end{enumerate}
    \item Zero padding
    \begin{itemize}
        \item Among input values, border values are less considered then other values
        \item Add extra paddings of 0 on every side of matrix, with width $W_p$ and height $H_p$
        \item Border values are fully considered, weights of extra pads are vanished($\times 0$)
        \item Output shape
        \begin{equation}
            y:~C\times\left(W_o=\frac{W_i+2W_p-W_f}{S}+1\right)\times\left(H_o=\frac{H_i+2H_p-H_f}{S}+1\right)
        \end{equation}
    \end{itemize}
    \item \# of learnable parameters: $C(W_fH_fD+1)$
\end{itemize}

\subsubsection*{Simple CNN Example for Single Filter}
\begin{itemize}
    \item Suppose an input data $x\in\mathbb{R}^{2\times5\times5}$
    \begin{equation}
        x=\left(\begin{mtx}{ccccc}
            3 & 3 & 6 & -2 & -5\\
            0 & 4 & 6 & 6 & -5\\
            5 & -4 & 6 & 0 & 1\\
            -3 & -2 & -5 & 5 & -2\\
            1 & -4 & 4 & -1 & -3
        \end{mtx},\begin{mtx}{ccccc}
            5 & 2 & 3 & 5 & -5\\
            5 & -4 & -3 & 5 & 5\\
            3 & 0 & -1 & 6 & -3\\
            3 & 5 & 0 & 3 & 3\\
            5 & 4 & 3 & 5 & 6
        \end{mtx}\right)
    \end{equation}
    \item Suppose a weight matrix of one filter $W\in\mathbb{R}^{2\times3\times3}$ and bias $b=1$
    \begin{equation}
        W=\left(\begin{mtx}{cccc}
            0 & -3 & 3\\
            3 & 2 & 0\\
            3 & 2 & 6
        \end{mtx},\begin{mtx}{cccc}
            4 & 6 & -1\\
            -2 & -1 & 2\\
            -4 & 2 & 0
        \end{mtx}\right)
    \end{equation}
    \item On $S=1$ and no padding, shape of output $y$ is: $(1,3,3)=(3,3)$
    \item Reshaped $W$, $\tilde{w}$ is
    \begin{equation}
        \tilde{w}=(0,-3, 3, 3, 2, 0, 3, 2, 6, 4,6,-1,-2,-1,2,-4,2,0)
    \end{equation}
    \item Compute $y_{11}$
    \begin{equation}
        x=\left(\begin{mtx}{ccccc}
            \textcolor{red}{3} & \textcolor{red}{3} & \textcolor{red}{6} & -2 & -5\\
            \textcolor{red}{0} & \textcolor{red}{4} & \textcolor{red}{6} & 6 & -5\\
            \textcolor{red}{5} & \textcolor{red}{-4} & \textcolor{red}{6} & 0 & 1\\
            -3 & -2 & -5 & 5 & -2\\
            1 & -4 & 4 & -1 & -3
        \end{mtx},\begin{mtx}{ccccc}
            \textcolor{red}{5} & \textcolor{red}{2} & \textcolor{red}{3} & 5 & -5\\
            \textcolor{red}{5} & \textcolor{red}{-4} & \textcolor{red}{-3} & 5 & 5\\
            \textcolor{red}{3} & \textcolor{red}{0} & \textcolor{red}{-1} & 6 & -3\\
            3 & 5 & 0 & 3 & 3\\
            5 & 4 & 3 & 5 & 6
        \end{mtx}\right)
    \end{equation}
    \begin{itemize}
        \item Extract submatrix from overlapping area with $W$, then reshape to $\tilde{x}$
        \begin{equation}
            \tilde{x}=(3,3,6,0,4,6,5,-4,6,5,2,3,5,-4,-3,3,0,-1)
        \end{equation}
        \item Then, $y_{11}=\tilde{w}^T\tilde{x}+b=66$
    \end{itemize}
    \item Compute $y_{12}$
    \begin{equation}
        x=\left(\begin{mtx}{ccccc}
            3 & \textcolor{blue}{3} & \textcolor{blue}{6} & \textcolor{blue}{-2} & -5\\
            0 & \textcolor{blue}{4} & \textcolor{blue}{6} & \textcolor{blue}{6} & -5\\
            5 & \textcolor{blue}{-4} & \textcolor{blue}{6} & \textcolor{blue}{0} & 1\\
            -3 & -2 & -5 & 5 & -2\\
            1 & -4 & 4 & -1 & -3
        \end{mtx},\begin{mtx}{ccccc}
            5 & \textcolor{blue}{2} & \textcolor{blue}{3} & \textcolor{blue}{5} & -5\\
            5 & \textcolor{blue}{-4} & \textcolor{blue}{-3} & \textcolor{blue}{5} & 5\\
            3 & \textcolor{blue}{0} & \textcolor{blue}{-1} & \textcolor{blue}{6} & -3\\
            3 & 5 & 0 & 3 & 3\\
            5 & 4 & 3 & 5 & 6
        \end{mtx}\right)
    \end{equation}
    \begin{itemize}
        \item \textit{Slide} $W$ by $S=1$
        \item Extract submatrix from overlapping area with $W$, then reshape to $\tilde{x}$
        \begin{equation}
            \tilde{x}=(3,6,-2,4,6,6,-4,6,0,2,3,5,-4,-3,5,0,-1,6)
        \end{equation}
        \item Then, $y_{12}=\tilde{w}^T\tilde{x}+b=41$
    \end{itemize}
    \item Repeating the process gives output
    \begin{equation}
        y=\begin{mtx}{ccc}
            66 & 41 & 120\\
            -40 & -31 & -22\\
            27 & -69 & 27
        \end{mtx}
    \end{equation}
\end{itemize}

\subsubsection*{Hyperparameters of Convolutional Layer}
\begin{itemize}
    \item Channel size($C$): Provides various perspectives of input data
    \begin{itemize}
        \item Typically powers of 2
    \end{itemize}
    \item Spatial extent($W_f$, $H_f$): Receptive field, spatial information of input
    \begin{itemize}
        \item Typically odd value, $W_f=H_f$
        \item $W_f$, $H_f$ shrinks input; Too fast shrink is not good
    \end{itemize}
    \item Stride($S$)
    \begin{itemize}
        \item Typically 1, $S\mid W_i+2W_p-W_f,S\mid H_i+2H_p-H_f$
        \item $S$ shrinks input; Too fast shrink is not good
    \end{itemize}
    \item Padding size($W_p$, $H_p$): Typically 0 or $W_p=\frac{1}{2}(W_f-1)$, $H_p=\frac{1}{2}(H_f-1)$
\end{itemize}

\subsubsection*{Convolutional Layer and FC Layer}
\begin{itemize}
    \item FC layer is a special case of Conv. Layer, i.e. $W_f=W_i$, $H_f=H_i$
    \item Conv. layer has less params $\implies$ Easy to optimize
    \item Provides attention on \textit{local patterns}
    \item Enables hierarchial and deep structure
    \item Conv. layer is translation-invariance, i.e. Absolute position of visual features doesn't affect output
\end{itemize}

\subsubsection*{Pooling Layer}
\begin{itemize}
    \item Aggresive downsampling $\to$ Smaller, more manageable representations
    \item Pooling methods may vary; e.g. max pool, average pool, etc
    \item Typically, conv. layer with $C=D$, $W_f=H_f=2$ or $3$, $S=2$, $W_p=H_p=0$
    \item Consider an input $x\in\mathbb{R}^{2\times4\times4}$
    \begin{equation}
        x=\left(\begin{mtx}{cccc}
            6 & -2 & -4 & 6\\
            -1 & 5 & 1 & 1\\
            4 & -2 & -5 & 0\\
            0 & -2 & -2 & -5
        \end{mtx},\begin{mtx}{cccc}
            3 & 2 & 2 & -4\\
            2 & -1 & 3 & 4\\
            5 & -5 & -5 & 4\\
            2 & 3 & -1 & -3
        \end{mtx}\right)
    \end{equation}
    \item With $2\times2$ filter and stride $2$, max pool result $y_{\text{max}}$ and average pool result $y_{\text{avg}}$ are
    \begin{equation}
        y_{\text{max}}=\left(\begin{mtx}{cc}
            6 & 6\\
            4 & 0
        \end{mtx},\begin{mtx}{cc}
            3 & 4\\
            5 & 4
        \end{mtx}\right),~y_{\text{avg}}=\left(\begin{mtx}{cc}
            2 & 1\\
            0 & -3
        \end{mtx},\begin{mtx}{cc}
            1.5 & 1.25\\
            1.25 & -1.25
        \end{mtx}\right)
    \end{equation}
    \item Pooling layers are often inserted between some Conv.-ReLU layers
\end{itemize}
