\section{Neural Networks}

\subsection{Artificial Neural Networks}

\subsubsection*{Motivation}
\begin{itemize}
    \item Pros and cons of linear classifier
    \begin{itemize}
        \item Pros: Simple $\Rightarrow$ Very expressive
        \item Cons: Not very powerful (e.g. XOR problem)
    \end{itemize}
    \item Human's thinking process is hierachial (low level $\rightarrow$ high level)
    \begin{itemize}
        \item e.g. Book reading: letters $\rightarrow$ words $\rightarrow$ sentences $\rightarrow$ story
    \end{itemize}
    \item Insert non-linear models between linear models $\Rightarrow$ \textbf{Deep Learning}
    \item Non-linear function $f$ is called ``activation function''
    \begin{itemize}
        \item $f$ is elementwise, i.e. $f\left((x_1,x_2,\cdots,x_n)\right)=(f(x_1),f(x_2),\cdots,f(x_n))$
    \end{itemize}
\end{itemize}
\begin{figures}
    \subfig{Nonlinear models}{nonlinear-models.png}{.3808}
    \subfig{Activation function}{activation-function.png}{.4192}
\end{figures}

\subsubsection*{Multilayer Neural Network}
\begin{itemize}
    \item $n$-layer neural network, or $(n-1)$-hidden-layer neural network
    \item 1 input layer $\rightarrow$ $(n-1)$ hidden layer $\rightarrow$ 1 output layer
    \item Scores on hidden layer are called ``intermediate output''
    \item Every hidden \& output layer has non-linear function $f$
    \begin{itemize}
        \item If not, it can be merged to linear model
    \end{itemize}
    \item Fully connected(FC) layers: Every layers are connected each other
\end{itemize}
\begin{figures}
    \fig{multilayer-neural-network.png}{.45}
\end{figures}
\clearpage

\subsubsection*{Computing Neural Network}
\begin{itemize}
    \item Consider $k$-th(input) layer has $m$ classes $\Rightarrow$ Scores $x=x_1,x_2,\cdots,x_m$
    \item Consider $(k+1)$-th(output) layer has $n$ classes $\Rightarrow$ Scores $y=y_1,y_2,\cdots,y_n$
    \item Let $w_{ij}$ be the parameter from $i$-th input to $j$-th output, $w_j=(w_{1j},w_{2j},\cdots,w_{mj})$
    \item Let $b_j$ be the bias from input layer to $j$-th output
    \item Let $f$ be the activation function on output layer
    \item Then, value of $j$-th output is
    \begin{equation}
        y_j=f(w_{1j}x_1+w_{2j}x_2+\cdots+w_{ij}x_i+b_j)=f(w_j^Tx+b_j)
    \end{equation}
    \item Let $W=\left(w_1^T,w_2^T,\cdots,w_n^T\right)$ and $b=(b_1,b_2,\cdots,b_n)$, then
    \begin{equation}
        y=f(Wx+b),\text{ i.e. }\begin{mtx}{c}
            y_1 \\ y_2 \\ \vdots \\ y_n
        \end{mtx}=f\left(\begin{mtx}{cccc}
            w_{11} & w_{21} & \cdots & w_{m1}\\
            w_{12} & w_{22} & \cdots & w_{m2}\\
            \vdots & \vdots & \ddots & \vdots\\
            w_{1n} & w_{2n} & \cdots & w_{mn}
        \end{mtx}\begin{mtx}{c}
            x_1 \\ x_2 \\ \vdots \\ x_m
        \end{mtx}+\begin{mtx}{c}
            b_1 \\ b_2 \\ \vdots \\ b_n
        \end{mtx}\right)
    \end{equation}
    \item For $k$-layer nerual network, $y=f_k(W_kf_{k-1}(W_{k-1}\cdots f_1(W_1x)))$
\end{itemize}
\begin{figures}
    \fig{neural-network-calculation.png}{.25}
\end{figures}

\subsubsection*{Activation Functions}
\begin{itemize}
    \item ReLU (Rectified Linear Unit)
    \begin{equation}
        f(x)=\max\{0,x\}
    \end{equation}
    \begin{itemize}
        \item Filters out $x<0$ $\Rightarrow$ rectifies information
        \item Simple calculation, similar to linear $\Rightarrow$ 표현력과 최적화 모두 좋음 $\Rightarrow$ Good choice for many problems
    \end{itemize}
    \item Sigmoid ($\sigma(x)=\frac{1}{1+e^{-x}}$), Hyper-tangent ($f(x)=\tanh(x)$)
    \item Maxout ($f(x)=\max\{w_1^Tx+b_1,w_2^Tx+b_2\}$)
    \item Leaky ReLU ($f(x)=\max\{\alpha x,x\}~(\alpha>0)$), ELU ($f(x)=x~(x\geq 0),~\alpha(e^x-1)~(x<0)$)
\end{itemize}
\begin{figures}
    \subfig{ReLU}{actfunc-relu.jpg}{.18}
    \subfig{Sigmoid}{actfunc-sigmoid.jpg}{.18}
    \subfig{tanh}{actfunc-tanh.jpg}{.18}
    \subfig{Leaky ReLU}{actfunc-leaky-relu.jpg}{.18}
    \subfig{ELU}{actfunc-elu.jpg}{.18}
\end{figures}

\subsection{Optimizing Loss Functions}

\subsubsection*{Optimization}
\begin{itemize}
    \item Optimization: to minimize or maximize a function $f:\mathbb{R}^n\to\mathbb{R}$
    \item $f(x)$ is minimum at $x=x^\ast$ $\Rightarrow$ $x^\ast$: optimal point, $f(x^\ast)$: optimal value
    \begin{itemize}
        \item $x=x^\ast$ is (global) optimal: $\forall x\in\mathcal{D}(f)$, $f(x^\ast)\leq f(x)$
        \item $x=x^\ast$ is local optimal: $\exists~R>0$ s.t. $x^\ast$ is optimal point for $f$ at $\Vert x-x^\ast\Vert\leq R$
    \end{itemize}
    \item Optimization methods
    \begin{itemize}
        \item If $\exists~f^\prime$, all optimal points $x=x^\ast$ satisfies $\nabla f(x^\ast)=0$
        \item Produce a sequence of points $x^{(k)}$ for $k=1,2,\cdots$ such that $f\left(x^{(k)}\right)\to f\left(x^\ast\right)$
    \end{itemize}
\end{itemize}

\subsubsection*{Gradient}
\begin{itemize}
    \item Gradient is a vector of partial derivatives of function $f:\mathbb{R}^n\rightarrow\mathbb{R}$
    \begin{equation}
        \nabla f(x)=\left(\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2},\cdots,\frac{\partial f(x)}{\partial x_n}\right)
    \end{equation}
    \item Taylor expansion: $f(x + \Delta x) \approx f(x) + \nabla f(x)^T \Delta x$
    \item Example
    \begin{itemize}
        \item $f(x)=c^Tx$ $\Rightarrow$ $\nabla f(x)=c$
        \item $f(x)=x^TQx$ $\Rightarrow$ $\nabla f(x)=2Qx$
    \end{itemize}
    \item Gradient vector $\nabla f$ points in the direction of greatest increase/decrease of $f$
    \begin{itemize}
        \item Directional derivative $\nabla f(x)^T\Delta x$: Rate of change to direction $\Delta x$
        \item For any $\Vert\Delta x\Vert=1$, $f(x)$ increases the most if $\Delta x=\frac{\nabla f(x)}{\Vert\nabla f(x)\Vert}$
        \item For any $\Vert\Delta x\Vert=1$, $f(x)$ decreases the most if $\Delta x=-\frac{\nabla f(x)}{\Vert\nabla f(x)\Vert}$
    \end{itemize}
\end{itemize}

\subsubsection*{Gradient Descent (GD) Method}
\begin{itemize}
    \item Algorithm: General descent method with $\Delta x = -\nabla f(x)$
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, tolerance $\epsilon>0$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Determine a descent direction $\Delta x=-\nabla f(x)$
            \item \textit{Line search} Choose a step size $\alpha>0$
            \item \textit{Update} $x:=x+\alpha\Delta x$
        \end{enumerate}
        \item[until] stopping criterion is satisfied; usually $\Vert\nabla f(x)\Vert_2\leq\epsilon$
    \end{description}
    \item Pros of GD
    \begin{itemize}
        \item Convergence result: $f(x^{(k)})-p^\ast\leq c^k\left(f(x^{(0)})-p^\ast\right)$, $c\in(0,1)$
        \item Convergence is guaranteed and mostly very fast ($\because$ exponential)
        \item Most cases, simple and efficient
    \end{itemize}
    \item Cons of GD
    \begin{itemize}
        \item Myopic algorithm: Can be very slow for some problems ($c\to 1$)
        \item GD may find a local optimal, but not necessarily global optimal
    \end{itemize}
\end{itemize}

\subsubsection*{GD Algorithm for Optimizing Loss Functions}
\begin{itemize}
    \item Objective of deep learning is to optimize loss function $L(W)$
    \item By applying GD algorithm, repeat
    \begin{equation}
        W\leftarrow W-\alpha\nabla L(W)
    \end{equation}
    \item GD may fail to find global optimal
    \begin{itemize}
        \item But, local optimal values of neural networks are similar
        \item Not a big problem at neural networks
    \end{itemize}
    \item Directly computing $\nabla L(W)$ by differentiation is complicated! $\Rightarrow$ Backpropagation
\end{itemize}

\subsection{Backpropagation}

\subsubsection*{Jacobian}
\begin{itemize}
    \item Partial derivative of multidimensional mapping
    \item $x\in\mathbb{R}^m$, $y\in\mathbb{R}^n$ $\implies$ $\frac{\partial y}{\partial x}:m\times n$ matrix
    \begin{equation}
        \frac{\partial y}{\partial x}=\begin{mtx}{cccc}
            \nabla y_1(x)^T \\
            \nabla y_2(x)^T \\
            \vdots \\
            \nabla y_m(x)^T
        \end{mtx}=\begin{mtx}{cccc}
            \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n}\\
            \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n}\\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n}
        \end{mtx}
    \end{equation}
    \item $X\in\mathbb{R}^{m\times n}$, $y\in\mathbb{R}$ $\implies$ $\frac{\partial y}{\partial X}:m\times n$ matrix
    \begin{equation}
        \frac{\partial y}{\partial X}=\begin{mtx}{cccc}
            \frac{\partial y}{\partial x_{11}} & \frac{\partial y}{\partial x_{12}} & \cdots & \frac{\partial y}{\partial x_{1n}}\\
            \frac{\partial y}{\partial x_{21}} & \frac{\partial y}{\partial x_{22}} & \cdots & \frac{\partial y}{\partial x_{2n}}\\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial y}{\partial x_{m1}} & \frac{\partial y}{\partial x_{m2}} & \cdots & \frac{\partial y}{\partial x_{mn}}
        \end{mtx}
    \end{equation}
    \item $X\in\mathbb{R}^{m\times n}$, $y\in\mathbb{R}^k$ $\implies$ $\frac{\partial y}{\partial X}:k\times m\times n$ matrix(=tensor)
    \begin{equation}
        \frac{\partial y}{\partial X}=\left(\begin{mtx}{ccc}
            \frac{\partial y_1}{\partial x_{11}} & \cdots & \frac{\partial y_1}{\partial x_{1n}}\\
            \vdots & \ddots & \vdots \\
            \frac{\partial y_1}{\partial x_{m1}} & \cdots & \frac{\partial y_1}{\partial x_{mn}}
        \end{mtx},\begin{mtx}{ccc}
            \frac{\partial y_2}{\partial x_{11}} & \cdots & \frac{\partial y_2}{\partial x_{1n}}\\
            \vdots & \ddots & \vdots \\
            \frac{\partial y_2}{\partial x_{m1}} & \cdots & \frac{\partial y_2}{\partial x_{mn}}
        \end{mtx},\cdots,\begin{mtx}{ccc}
            \frac{\partial y_k}{\partial x_{11}} & \cdots & \frac{\partial y_k}{\partial x_{1n}}\\
            \vdots & \ddots & \vdots \\
            \frac{\partial y_k}{\partial x_{m1}} & \cdots & \frac{\partial y_k}{\partial x_{mn}}
        \end{mtx}\right)
    \end{equation}
    \item Jacobian is a \textit{transpose} of gradient
    \item Chain rule: For $y=h(g(f(x)))$,
    \begin{equation}
        \frac{\partial y}{\partial x}=\frac{\partial y}{\partial g}\frac{\partial g}{\partial f}\frac{\partial f}{\partial x}
    \end{equation}
    \begin{itemize}
        \item Sequence of multiplication is important
    \end{itemize}
\end{itemize}
\clearpage

\subsubsection*{Backpropagation}
\begin{itemize}
    \item Computational Graph: Computation relations expressed in graph
    \begin{figures}
        \fig{computational-graph.png}{.7}
    \end{figures}
    \begin{itemize}
        \item Computational graph of $f(w,x)=\sigma(w_0x_0+w_1x_1+w_2)$
        \item Each node represents function (e.g. add., mult., exponential, sigmoid, etc.)
    \end{itemize}
    \item Forward computation: Compute loss along graph from given input values
    \item Backpropagation: Use chain rule and gradient from next node to compute previous nodes' gradients
    \begin{itemize}
        \item Suppose a node: input $x$ and $y$, output $z$, function $f$, i.e. $z=f(x,y)$
        \item Let $L$ be a loss of graph
        \item Upstream gradient(s): $\frac{\partial L}{\partial z}$; given from next node
        \begin{itemize}
            \item Upstream gradient of last node is always $1$
        \end{itemize}
        \item Local gradient(s): $\frac{\partial z}{\partial x}$, $\frac{\partial z}{\partial y}$; can be computed from function $f$ and input/output values of the node
        \item Use chain rule to compute $\frac{\partial L}{\partial x}$, $\frac{\partial L}{\partial y}$
        \begin{equation}
            \frac{\partial L}{\partial x}=\frac{\partial L}{\partial z}\frac{\partial z}{\partial x},~\frac{\partial L}{\partial y}=\frac{\partial L}{\partial z}\frac{\partial z}{\partial y}
        \end{equation}
        \item Computed gradients are \textit{propagated} to previous layers $\implies$ \textbf{Backpropagation}
    \end{itemize}
    \item Patterns in backward flow
    \begin{itemize}
        \item add gate is gradient ``distributor'': propagates to previous as it is
        \item max gate is gradient ``router'': only propagates to max value node
        \item mult gate is gradient ``switcher'': inputs switch, then propagates with upstream gradient
    \end{itemize}
    \item Nodes may have branches, i.e. multiple branches
    \begin{equation}
        y=f(x),~z=g(x)\implies\frac{\partial L}{\partial x}=\frac{\partial L}{\partial y}\frac{\partial y}{\partial x}+\frac{\partial L}{\partial z}\frac{\partial z}{\partial x}
    \end{equation}
    \item Optimizing computation
    \begin{itemize}
        \item Matrix multiplication is \textit{expensive} for huge matrices
        \item Consider $y=f(x)=\max\{x,0\}$, then
        \begin{equation}
            \frac{\partial L}{\partial x_{ij}}=\begin{cases}
                \frac{\partial L}{\partial y_{ij}} & (x_{ij}>0) \\
                0 & (x_{ij}\leq 0)
            \end{cases}
        \end{equation}
        \item Sometimes, element-wise computation is cheaper then matrix multiplication
    \end{itemize}
\end{itemize}

\subsection{Convolutional Neural Networks}

\subsubsection*{Limitations of ANN with FC layers}
\begin{itemize}
    \item On image recognition, spatial information(local structures) are important
    \item In FC layers, spatial information is lost!
    \item By subsampling input images, get spatial information $\implies$ \textit{convolution}
\end{itemize}
\begin{figures}
    \fig{cnn.png}{.5}
\end{figures}

\subsubsection*{Convolutional Layer}
\begin{itemize}
    \item Suppose input data(image) $x$ is
    \begin{equation}
        x:~W_i\times H_i\times D
    \end{equation}
    \item Suppose \# of channels $C\in\mathbb{N}$, stride $S\in\mathbb{N}$
    \item Suppose $c$-th filter and $c$-th bias $W_c$ and $b_c\in\mathbb{R}$
    \begin{equation}
        W_c:~W_f\times H_f\times D~(W_f\leq W_i,~H_f\leq H_i)
    \end{equation}
    \item Shape of output data
    \begin{equation}
        y:~\left(W_o=\frac{W_i-W_f}{S}+1\right)\times\left(H_o=\frac{H_i-H_f}{S}+1\right)\times C
    \end{equation}
    \item Computing output data
    \begin{equation}
        y_{w_o,h_o,c}=\sum_{d=1}^D\sum_{h_f=1}^{H_f}\sum_{w_f=1}^{W_f}W_{c,w_f,h_f,d}\cdot x_{(w_0-1)S+w_f,(h_0-1)S+h_f,d}+b_c
    \end{equation}
    \begin{enumerate}
        \item Reshape $W_c$ to a vector $\tilde{w}$
        \item Extract a $W_f\times H_f\times D$ matrix from $W_c$ to a vector $\tilde{x}$
        \item Compute inner product: $y_{w_o,h_o,c}=\tilde{w}^T\tilde{x}+b_c$
        \item Slide the filter by stride $S$, and repeat the process
        \item Repeat the process by every channel
    \end{enumerate}
    \item Zero padding
    \begin{itemize}
        \item Among input values, border values are less considered then other values
        \item Add extra paddings of 0 on every side of matrix, with width $W_p$ and height $H_p$
        \item Border values are fully considered, weights of extra pads are vanished($\times 0$)
        \item Input shape: $(W_i,H_i,D)\to(W_i+2W_p,H_i+2H_p,D)$
        \item Output shape
        \begin{equation}
            y:~\left(W_o=\frac{W_i+2W_p-W_f}{S}+1\right)\times\left(H_o=\frac{H_i+2H_p-H_f}{S}+1\right)\times C
        \end{equation}
    \end{itemize}
    \item \# of learnable parameters: $C(W_fH_fD+1)$
\end{itemize}

\subsubsection*{Hyperparameters of Convolutional Layer}
\begin{itemize}
    \item Channel size($C$): Provides various perspectives of input data
    \begin{itemize}
        \item Typically powers of 2
    \end{itemize}
    \item Spatial extent($W_f$, $H_f$): Receptive field, spatial information of input
    \begin{itemize}
        \item Typically odd value, $W_f=H_f$
        \item $W_f$, $H_f$ shrinks input; Too fast shrink is not good
    \end{itemize}
    \item Stride($S$)
    \begin{itemize}
        \item Typically 1, $\frac{W_i-W_f}{S},\frac{H_i-H_f}{S}\in\mathbb{N}$
        \item $S$ shrinks input; Too fast shrink is not good
    \end{itemize}
    \item Padding size($W_p$, $H_p$): Typically 0 or $W_p=\frac{1}{2}(W_f-1)$, $H_p=\frac{1}{2}(H_f-1)$
\end{itemize}

\subsubsection*{Convolutional Layer and FC Layer}
\begin{itemize}
    \item FC layer is a special case of Conv. Layer, i.e. $W_f=W_i$, $H_f=H_i$
    \item Conv. layer has less params $\implies$ Easy to optimize
    \item Provides attention on \textit{local patterns}
    \item Enables hierarchial and deep structure
    \item Conv. layer is translation-invariance, i.e. Absolute position of visual features doesn't affect output
\end{itemize}
