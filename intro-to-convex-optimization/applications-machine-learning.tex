\section{Applications: Machine Learning}

\subsection{Stochastic Gradient Descent}

\subsubsection*{Learning from Data}
\begin{itemize}
    \item Sample data (blue points) $(a_i, b_i)$: $a_i$ is input, $b_i$ is output
    \item Assume $b=f(a)+\epsilon$; $f$: unknown function, $\epsilon$: random error
    \item Estimation (red line) $\hat{f}(a;x)\approx f(a)$, $x$: some parameters
\end{itemize}
\begin{figures}
    \fig{intro-to-convex-optimization/images/learning-from-data.PNG}{.5}
\end{figures}

\subsubsection*{Linear Regression}
\begin{itemize}
    \item Linear explanation
        $$ b_i\approx\hat{f}(a_i;x)=x_0+x_1a_{i,1}+x_2a_{i,2}+\cdots+x_na_{i,n}=\tilde{a}_i^Tx $$
    \begin{itemize}
        \item Independent (explanatory, input) variable: $a_1,a_2,\cdots,a_m\in\mathbb{R}^n$
        \item Dependent (observation, output) variable: $b_1,b_2,\cdots,b_m$
        \item $x_0$: bias, $\tilde{a}_i=(1,a_i)$와 같이 parameter로 간주됨 $\Rightarrow$ Linear combination of $\mathbb{R}^{n+1}$
    \end{itemize}
    \item Objective: Minimize Loss function $L$
        $$ L(x)=\frac{1}{m}\sum_{i=1}^{m}\left(\hat{f}(a_i;x)-b_i\right)^2 = \frac{1}{m}\Vert Ax-b\Vert_2^2 $$
    \begin{itemize}
        \item Mean Square Error (MSE), Adjust $x$ to minimize $L$ $\Rightarrow$ QP for MSE
        \item Linear explanation $\hat{f}(a_i;x)=\tilde{a}_i^Tx$ $\Rightarrow$ Linear regression
        \item $\hat{f}(a_i;x)$: estimation, $b_i$: true explanation
        \item Solution: $\nabla L(x^\ast)=0$ $\Rightarrow$ $x^\ast=(A^TA)^{-1}A^Tb$
    \end{itemize}
\end{itemize}

\subsubsection*{Stochastic Gradient Descent (SGD)}
\begin{itemize}
    \item Motivation
    \begin{itemize}
        \item Machine Learning: Accepts very large dataset and number of variables
        \item Assume linear regression $L(x)=\frac{1}{m}\sum_{i=1}^{m}\left(\hat{f}(a_i;x)-b_i\right)^2 = \frac{1}{m}\Vert Ax-b\Vert_2^2$
        \item GD update: $x\leftarrow x-\gamma\frac{1}{m}\sum_{i=1}^{m}\nabla\left(\hat{f}(a_i;x)-b_i\right)^2=x-\gamma\frac{2}{m}\sum_{i=1}^ma_i(a_i^Tx-b_i)$
        \item $m$이 매우 크면 step별 계산이 너무 많아짐
    \end{itemize}
    \newpage
    \item Randomized, simplified version of GD (원래 GD는 Batch GD라고 불림)
    \item Method
    \begin{enumerate}
        \item Select \textbf{one} data sample randomly: choose $j$ from $1,2,\cdots,m$
        \item Update $x^{(k+1)}\leftarrow x^{(k)}+\gamma^{(k)}\nabla L_j(x^{(k)})$
    \end{enumerate}
    \item MSE minimization: $x\leftarrow x-2\gamma\left(\hat{f}(a_j;k)-b_j\right)\nabla\hat{f}(a_j;x)$
    \item $\nabla L_j$: a noisy, inaccurate estimate of true gradient $\nabla L$
    \begin{itemize}
        \item $\mathbb{E}_j[\nabla L_j(x)]=\nabla L(x)$
        \item an unbiased estimator, i.e. Long-term에서는 올바른 방향으로 가고 있음
        \item SGD may not converge, but oscillate nearby optimum
    \end{itemize}
    \item Single update의 연산 비용: GD는 $O(mn)$, SGD는 $O(n)$
    \item Step이 항상 descent direction은 아님
    \begin{itemize}
        \item Descent direction인지 확인하려면 $\nabla L$을 계산함 $\Rightarrow$ SGD 채택하는 의미가 없음
        \item Non-convex problem에서 global optimal이 아닌 local optimal에 빠지는 것을 방지
    \end{itemize}
    \item Convergence
    \begin{itemize}
        \item Suppose $f$: strongly convex, $\nabla f$: Lipschitz continuous (빠르게 변하지 않음)
        \item Batch GD: $f(x^{(k)})-p^\ast\leq O\left(c^k\right)$ ($c\in(0,1)$)
        \item SGD: $\mathbb{E}(f(x^{(k)}))-p^\ast\leq O\left(\frac{1}{k}\right)$ where $\sum_k\gamma_k=\infty$, $\sum_k\gamma_k<\infty$
        \item 수렴 속도는 Batch GD가 훨씬 빠름
    \end{itemize}
\end{itemize}

\subsubsection*{Mini-batch SGD}
\begin{itemize}
    \item Choose a random subset of dataset: Select $b$ samples, and use their mean gradient
    \begin{itemize}
        \item 메모리 접근의 효율성을 위해 대체로 $2^N$($N\in\mathbb{N}$)개 사용 (Typically, $N=5,6,\cdots,9$)
        \item $b=1$: SGD / $b=m$: Batch GD
    \end{itemize}
    \item Method
    \begin{enumerate}
        \item Select a subset of data sample randomly: choose $B\in\{1,2,\cdots,m\}$
        \item Update $x^{(k+1)}\leftarrow x^{(k)}+\gamma^{(k)}\frac{1}{|B|}\sum_{j\in B}\nabla L_j(x^{(k)})$
    \end{enumerate}
    \item Also an unbiased estimator of $\nabla L$ $\Rightarrow$ Long-term에서 올바른 방향으로 감
    \item Complexity per update: $O(bn)$
    \item Convergence: Lower variance as $|B|$ grows
    \item Mini-batch SGD is a sufficiently good solution
\end{itemize}
\begin{figures}
    \fig{intro-to-convex-optimization/images/sgd-graph.png}{.33}
\end{figures}

\subsection{Regularization}

\subsubsection*{Overfitting}
\begin{itemize}
    \item Consider a polynomial estimation $\hat{f}(a;x)=\sum_{i=0}^n\psi_i(a)x_i=\psi(a)^Tx$ where $\psi_i(a)=a^i$
    \item Suppose 10 samples (graphs below) in second-order polynomial relation
    \begin{itemize}
        \item Low order ($M=0,1$) does not fit the samples accurately
        \item Near order ($M=3$) represents the samples approximately
        \item High order ($M=9$) fits the samples perfectly, but does not represent the overall relation well
    \end{itemize}
    \item Overfitting: Sample에 지나치게 fit하는 model은 오히려 모집단의 성격을 나타내지 못할 수 있음
\end{itemize}
\begin{figures}
    \subfig{intro-to-convex-optimization/images/overfitting-order-0.png}{.23}
    \subfig{intro-to-convex-optimization/images/overfitting-order-1.png}{.23}
    \subfig{intro-to-convex-optimization/images/overfitting-order-3.png}{.23}
    \subfig{intro-to-convex-optimization/images/overfitting-order-9.png}{.23}
\end{figures}

\subsubsection*{Regularization}
\begin{itemize}
    \item Method to supress overfitting
    \begin{itemize}
        \item Overfitting의 현상: Parameters $x$ becomes very large
        \item Idea: supress overfitting by minimizing $\Vert x\Vert$
    \end{itemize}
    \item Problem becomes solving two optimization problems
    \begin{itemize}
        \item minimize $\Vert Ax-b\Vert_2$: Data Loss
        \item minimize $\Vert x\Vert_2$: Regulation Loss
    \end{itemize}
    \item Ridge regression: Multi-objective optimization
        $$ \begin{aligned}
            \mathrm{minimize}~~&~~\Vert Ax-b\Vert_2^2+\lambda\Vert x\Vert_2
        \end{aligned} $$
    \begin{itemize}
        \item Unconstrained QP
        \item $\lambda>0$: hyperparameter; $\lambda\ll1$: Data loss 중시; $\lambda\gg1$: Regulation loss 중시
        \item Optimality condition: $x=\left(A^TA+\lambda I\right)^{-1}A^Tb$
    \end{itemize}
    \begin{figures}
        \subfig{intro-to-convex-optimization/images/regularization-small-lambda.png}{.4}
        \subfig{intro-to-convex-optimization/images/regularization-large-lambda.png}{.4}
    \end{figures}
    \newpage
    \item $L_1$ Regularization: Supress $x$ by by L1-norm
        $$ \begin{aligned}
            \mathrm{minimize}~~&~~\Vert Ax-b\Vert_2^2+\lambda\Vert x\Vert_1 \\ {}
        \end{aligned}~~~\Leftrightarrow~~~\begin{aligned}
            \mathrm{minimize}~~&~~\Vert Ax-b\Vert_2^2+\lambda\sum t_i \\
            \mathrm{subject~to}~~&~~t_i\geq x_i,~t_i\geq -x_i
        \end{aligned} $$
    \begin{itemize}
        \item Equivalent to a QP by introducing epigraph
        \item Solution often has zero entries $\Rightarrow$ ``Sparse'' $x$ is oftenly obtained (Image below)
        \item Often used to approximate $L_0$ regularization ($\Vert x\Vert_0$: \# of nonzero elements in $x$; nonconvex)
    \end{itemize}
    \begin{figures}
        \subfig{intro-to-convex-optimization/images/l1-norm.png}{.23}
        \subfig{intro-to-convex-optimization/images/l2-norm.png}{.23}
    \end{figures}
\end{itemize}
