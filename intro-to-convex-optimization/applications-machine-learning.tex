\section{Applications: Machine Learning}

\subsection{Stochastic Gradient Descent}

\subsubsection*{Learning from Data}
\begin{itemize}
    \item Sample data (blue points) $(a_i, b_i)$: $a_i$ is input, $b_i$ is output
    \item Assume $b=f(a)+\epsilon$; $f$: unknown function, $\epsilon$: random error
    \item Estimation (red line) $\hat{f}(a;x)\approx f(a)$, $x$: some parameters
\end{itemize}
\begin{figures}
    \fig{intro-to-convex-optimization/images/learning-from-data.PNG}{.5}
\end{figures}

\subsubsection*{Linear Regression}
\begin{itemize}
    \item Linear explanation
        $$ b_i\approx\hat{f}(a_i;x)=x_0+x_1a_{i,1}+x_2a_{i,2}+\cdots+x_na_{i,n}=\tilde{a}_i^Tx $$
    \begin{itemize}
        \item Independent (explanatory, input) variable: $a_1,a_2,\cdots,a_m\in\mathbb{R}^n$
        \item Dependent (observation, output) variable: $b_1,b_2,\cdots,b_m$
        \item $x_0$: bias, $\tilde{a}_i=(1,a_i)$와 같이 parameter로 간주됨 $\Rightarrow$ Linear combination of $\mathbb{R}^{n+1}$
    \end{itemize}
    \item Objective: Minimize Loss function $L$
        $$ L(x)=\frac{1}{m}\sum_{i=1}^{m}\left(\hat{f}(a_i;x)-b_i\right)^2 = \frac{1}{m}\Vert Ax-b\Vert_2^2 $$
    \begin{itemize}
        \item Mean Square Error (MSE), Adjust $x$ to minimize $L$ $\Rightarrow$ QP for MSE
        \item Linear explanation $\hat{f}(a_i;x)=\tilde{a}_i^Tx$ $\Rightarrow$ Linear regression
        \item $\hat{f}(a_i;x)$: estimation, $b_i$: true explanation
        \item Solution: $\nabla L(x^\ast)=0$ $\Rightarrow$ $x^\ast=(A^TA)^{-1}A^Tb$
    \end{itemize}
\end{itemize}

\subsubsection*{Stochastic Gradient Descent (SGD)}
\begin{itemize}
    \item Motivation
    \begin{itemize}
        \item Machine Learning: Accepts very large dataset and number of variables
        \item Assume linear regression $L(x)=\frac{1}{m}\sum_{i=1}^{m}\left(\hat{f}(a_i;x)-b_i\right)^2 = \frac{1}{m}\Vert Ax-b\Vert_2^2$
        \item GD update: $x\leftarrow x-\gamma\frac{1}{m}\sum_{i=1}^{m}\nabla\left(\hat{f}(a_i;x)-b_i\right)^2=x-\gamma\frac{2}{m}\sum_{i=1}^ma_i(a_i^Tx-b_i)$
        \item $m$이 매우 크면 step별 계산이 너무 많아짐
    \end{itemize}
    \newpage
    \item Randomized, simplified version of GD (원래 GD는 Batch GD라고 불림)
    \item Method
    \begin{enumerate}
        \item Select \textbf{one} data sample randomly: choose $j$ from $1,2,\cdots,m$
        \item Update $x^{(k+1)}\leftarrow x^{(k)}+\gamma^{(k)}\nabla L_j(x^{(k)})$
    \end{enumerate}
    \item MSE minimization: $x\leftarrow x-2\gamma\left(\hat{f}(a_j;k)-b_j\right)\nabla\hat{f}(a_j;x)$
    \item $\nabla L_j$: a noisy, inaccurate estimate of true gradient $\nabla L$
    \begin{itemize}
        \item $\mathbb{E}_j[\nabla L_j(x)]=\nabla L(x)$
        \item an unbiased estimator, i.e. Long-term에서는 올바른 방향으로 가고 있음
        \item SGD may not converge, but oscillate nearby optimum
    \end{itemize}
    \item Single update의 연산 비용: GD는 $O(mn)$, SGD는 $O(n)$
    \item Step이 항상 descent direction은 아님
    \begin{itemize}
        \item Descent direction인지 확인하려면 $\nabla L$을 계산함 $\Rightarrow$ SGD 채택하는 의미가 없음
        \item Non-convex problem에서 global optimal이 아닌 local optimal에 빠지는 것을 방지
    \end{itemize}
    \item Convergence
    \begin{itemize}
        \item Suppose $f$: strongly convex, $\nabla f$: Lipschitz continuous (빠르게 변하지 않음)
        \item Batch GD: $f(x^{(k)})-p^\ast\leq O\left(c^k\right)$ ($c\in(0,1)$)
        \item SGD: $\mathbb{E}(f(x^{(k)}))-p^\ast\leq O\left(\frac{1}{k}\right)$ where $\sum_k\gamma_k=\infty$, $\sum_k\gamma_k^2<\infty$
        \item 수렴 속도는 Batch GD가 훨씬 빠름
    \end{itemize}
\end{itemize}

\subsubsection*{Mini-batch SGD}
\begin{itemize}
    \item Choose a random subset of dataset: Select $b$ samples, and use their mean gradient
    \begin{itemize}
        \item 메모리 접근의 효율성을 위해 대체로 $2^N$($N\in\mathbb{N}$)개 사용 (Typically, $N=5,6,\cdots,9$)
        \item $b=1$: SGD / $b=m$: Batch GD
    \end{itemize}
    \item Method
    \begin{enumerate}
        \item Select a subset of data sample randomly: choose $B\in\{1,2,\cdots,m\}$
        \item Update $x^{(k+1)}\leftarrow x^{(k)}+\gamma^{(k)}\frac{1}{|B|}\sum_{j\in B}\nabla L_j(x^{(k)})$
    \end{enumerate}
    \item Also an unbiased estimator of $\nabla L$ $\Rightarrow$ Long-term에서 올바른 방향으로 감
    \item Complexity per update: $O(bn)$
    \item Convergence: Lower variance as $|B|$ grows
    \item Mini-batch SGD is a sufficiently good solution
\end{itemize}
\begin{figures}
    \fig{intro-to-convex-optimization/images/sgd-graph.png}{.33}
\end{figures}

\subsection{Regularization}

\subsubsection*{Overfitting}
\begin{itemize}
    \item Consider a polynomial estimation $\hat{f}(a;x)=\sum_{i=0}^n\psi_i(a)x_i=\psi(a)^Tx$ where $\psi_i(a)=a^i$
    \item Suppose 10 samples (graphs below) in second-order polynomial relation
    \begin{itemize}
        \item Low order ($M=0,1$) does not fit the samples accurately
        \item Near order ($M=3$) represents the samples approximately
        \item High order ($M=9$) fits the samples perfectly, but does not represent the overall relation well
    \end{itemize}
    \item Overfitting: Sample에 지나치게 fit하는 model은 오히려 모집단의 성격을 나타내지 못할 수 있음
\end{itemize}
\begin{figures}
    \subfig{intro-to-convex-optimization/images/overfitting-order-0.png}{.23}
    \subfig{intro-to-convex-optimization/images/overfitting-order-1.png}{.23}
    \subfig{intro-to-convex-optimization/images/overfitting-order-3.png}{.23}
    \subfig{intro-to-convex-optimization/images/overfitting-order-9.png}{.23}
\end{figures}

\subsubsection*{Regularization}
\begin{itemize}
    \item Method to supress overfitting
    \begin{itemize}
        \item Overfitting의 현상: Parameters $x$ becomes very large
        \item Idea: supress overfitting by minimizing $\Vert x\Vert$
    \end{itemize}
    \item Problem becomes solving two optimization problems
    \begin{itemize}
        \item minimize $\Vert Ax-b\Vert_2$: Data Loss
        \item minimize $\Vert x\Vert_2$: Regulation Loss
    \end{itemize}
    \item Ridge regression: Multi-objective optimization
        $$ \begin{aligned}
            \mathrm{minimize}~~&~~\Vert Ax-b\Vert_2^2+\lambda\Vert x\Vert_2^2
        \end{aligned} $$
    \begin{itemize}
        \item Unconstrained QP
        \item $\lambda>0$: hyperparameter; $\lambda\ll1$: Data loss 중시; $\lambda\gg1$: Regulation loss 중시
        \item Optimality condition: $x=\left(A^TA+\lambda I\right)^{-1}A^Tb$
    \end{itemize}
    \begin{figures}
        \subfig{intro-to-convex-optimization/images/regularization-small-lambda.png}{.4}
        \subfig{intro-to-convex-optimization/images/regularization-large-lambda.png}{.4}
    \end{figures}
    \newpage
    \item $L_1$ Regularization: Supress $x$ by by L1-norm
        $$ \begin{aligned}
            \mathrm{minimize}~~&~~\Vert Ax-b\Vert_2^2+\lambda\Vert x\Vert_1 \\ {}
        \end{aligned}~~~\Leftrightarrow~~~\begin{aligned}
            \mathrm{minimize}~~&~~\Vert Ax-b\Vert_2^2+\lambda\sum t_i \\
            \mathrm{subject~to}~~&~~t\succeq x,~t\succeq -x
        \end{aligned} $$
    \begin{itemize}
        \item Equivalent to a QP by introducing epigraph
        \item Solution often has zero entries $\Rightarrow$ ``Sparse'' $x$ is oftenly obtained (Image below)
        \item Often used to approximate $L_0$ regularization ($\Vert x\Vert_0$: \# of nonzero elements in $x$; nonconvex)
    \end{itemize}
    \begin{figures}
        \subfig{intro-to-convex-optimization/images/l1-norm.png}{.2}
        \subfig{intro-to-convex-optimization/images/l2-norm.png}{.2}
    \end{figures}
\end{itemize}

\subsection{Classification}

\subsubsection*{Classification}
\begin{itemize}
    \item Assume $a_1$ characteristics are \textit{generally} classified into $c_1$, $a_2$ into $c_2$
    \item But, there is a chance $a_2$ characteristic corresponds to $c_1$, and vice versa
    \item Calculating probability of $a_1$ data being classified into $c_1$, $a_2$ into $c_2$ is critical
\end{itemize}

\subsubsection*{Logistic Regression}
\begin{itemize}
    \item Logistic regression provides \textbf{classifier}
        $$ \mathbb{P}(b=1)=\sigma(a^Tx)=\frac{1}{1+\exp(-a^Tx)} $$
    \begin{itemize}
        \item Binary classification: output $b$ is either $0$ or $1$
        \item $\hat{f}(a;x)\approx\mathbb{P}(b=1)$: probability of input $a$ classified into $b=1$; $\mathcal{R}(\hat{f})=[0,1]$
        \item Obtain `score' from linear regression $a^Tx$; $\mathcal{R}(a^Tx)=(-\infty,\infty)$
        \item Large score $\Rightarrow$ $\mathrm{prob}\approx1$, small score $\Rightarrow$ $\mathrm{prob}\approx0$
        \item Use sigmoid function $\sigma(t)=\frac{1}{1+\exp(-t)}$ to normalize $(-\infty,\infty)\rightarrow[0,1]$
    \end{itemize}
    \item Decision boundary: 판단을 내리는 기준점
    \begin{itemize}
        \item $\sigma(0)=0.5$를 기준으로 classification 결과를 정하는 것이 상식
        \item $a^Tx=0$, i.e. a hyperplane is a decision boundary
    \end{itemize}
\end{itemize}
\begin{figures}
    \subfig{intro-to-convex-optimization/images/logistic-regression.PNG}{.302}
    \subfig{intro-to-convex-optimization/images/sigmoid-function.PNG}{.328}
\end{figures}

\subsubsection*{Maximum Likelihood Estimation}
\begin{itemize}
    \item An example method determining $x$
    \item Consider Bernouli trials (e.g. biased coin toss) ($b$: result)
    \begin{itemize}
        \item $b=1$일 확률이 $\theta$, $b=0$일 확률이 $1-\theta$
        \item $b$를 observe할 확률: $\theta^b(1-\theta)^{1-b}$
        \item $m$번의 독립 시행으로 $b_1,b_2,\cdots,b_m\in\{0,1\}$을 관찰할 확률
            $$ \Lambda(\theta)=\prod_{i=1}^m\theta^{b_i}(1-\theta)^{1-b_i} $$
        \item $b=1$ 결과를 $k$회 관측 $\Rightarrow$ $\Lambda(\theta)=\theta^k(1-\theta)^{m-k}$
        \item Maximizing $\Lambda(\theta)$ is difficult; Maximize $\log\Lambda(\theta)$: log-likelihood
        \item $0=\frac\partial{\partial\theta}\log\Lambda(\theta)=\frac\partial{\partial\theta}\left(k\log\theta+(m-k)\log(1-\theta)\right)$ $\Rightarrow$ $\theta^\ast=\frac{k}{m}$
    \end{itemize}
    \item Applying Logistic Regression
    \begin{itemize}
        \item Observed data: $(a_i,b_i)$ ($i=1,2,\cdots,m$)
        \item Likelihood of observing these data:
            $$ \Lambda(x)=\prod_{i=1}^m\sigma(a_i^Tx)^{b_i}\left(1-\sigma(a_i^Tx)\right)^{1-b_i}=\prod_{i=1}^m\frac{\left(\exp(a_i^Tx)\right)^{b_i}}{1+\exp(a_i^Tx)} $$
        \item Log-likelihood:
            $$ \log\Lambda(x)=-\sum_{i=1}^m\log\left(1+\exp(a_i^Tx)\right)+\sum_{i\in\{i|b_i=1\}}a_i^Tx $$
        \item $\log\left(1+\exp(a_i^Tx)\right)$ is a log-sum-exponential of affine transform $\Rightarrow$ convex
            $$ y_i=\begin{mtx}{c}\mathbf{0}^T\\a_i^T\end{mtx}x=\begin{mtx}{c}0\\a_i^Tx\end{mtx}~\Rightarrow~\log\sum_{k=1}^2\exp y_{i,k}=\log\left(1+\exp(a_i^Tx)\right) $$
        \item Objective function is concave $\Rightarrow$ Convex optimization problem
    \end{itemize}
\end{itemize}

\subsubsection*{Linear Classifier}
\begin{itemize}
    \item Linear classifier
    \begin{itemize}
        \item Separating points into two sets: $X=\{x_1,x_2,\cdots,x_N\}$ and $Y=\{y_1,y_2,\cdots,y_M\}$
        \item 경계에 가장 가까운 $X$, $Y$의 점: $\hat{x}$, $\hat{y}$ $\Rightarrow$ $\exists~a,b$ s.t. $a^T\hat{x}+b=1$, $a^T\hat{y}+b=-1$
        \item $\forall~x\in X$, $a^Tx+b\geq1$, $\forall~y\in Y$, $a^Ty+b\leq-1$
        \item Boundary hyperplane $a^Tx+b=0$: $\hat{x}$, $\hat{y}$까지의 거리가 같음
    \end{itemize}
    \newpage
    \item Robust linear classifier
    \begin{itemize}
        \item In case the points can be separated into two sets completely (left image)
        \item Let $\mathcal{H}_1=\{z~|~a^Tz+b=1\}$, $\mathcal{H}_2=\{z~|~a^Tz+b=-1\}$
        \item Maximize the margin between two sets: $\mathrm{dist}(\mathcal{H}_1,\mathcal{H}_2)=\frac{2}{\Vert a\Vert_2}$
            $$ \begin{aligned}
                \mathrm{minimize}~~&~~\Vert a\Vert_2 \\
                \mathrm{subject~to}~~&~~a^Tx_i+b\geq1~~(i=1,2,\cdots,N) \\
                    &~~a^Tx_i+b\leq-1~~(i=1,2,\cdots,M)
            \end{aligned} $$
    \end{itemize}
    \item Approximate linear classifier
    \begin{itemize}
        \item In case the points cannot be separated into two sets completely (middle image)
        \item Objective: minimizing \# of missclassified points $\Rightarrow$ L0-norm $\Rightarrow$ Approximate to L1-norm
            $$ \begin{aligned}
                \mathrm{minimize}~~&~~\mathbf{1}^Tu+\mathbf{1}^Tv \\
                \mathrm{subject~to}~~&~~a^Tx_i+b\geq1-u_i~~(i=1,2,\cdots,N) \\
                    &~~a^Tx_i+b\leq-1+v_i~~(i=1,2,\cdots,M) \\
                    &~~u\succeq0,~v\succeq0
            \end{aligned} $$
        \item $u$, $v$ means \# of missclassified points
        \item At optimum, $u_i=\max\{0,1-a^Tx_i-b\}$, $v_i=\max\{0,1+a^Ty_i+b\}$
    \end{itemize}
    \item Support Vector Machine (SVM)
    \begin{itemize}
        \item Tradeoff between robust L.C. and approximate L.C. (right image)
        \item Introduces a hyperparameter $\gamma>0$
            $$ \begin{aligned}
                \mathrm{minimize}~~&~~\Vert a\Vert_2+\gamma(\mathbf{1}^Tu+\mathbf{1}^Tv) \\
                \mathrm{subject~to}~~&~~a^Tx_i+b\geq1-u_i~~(i=1,2,\cdots,N) \\
                    &~~a^Tx_i+b\leq-1+v_i~~(i=1,2,\cdots,M) \\
                    &~~u\succeq0,~v\succeq0
            \end{aligned} $$
    \end{itemize}
\end{itemize}
\begin{figures}
    \subfig{intro-to-convex-optimization/images/robust-linear-classifier.PNG}{.309}
    \subfig{intro-to-convex-optimization/images/approximate-linear-classifier.PNG}{.296}
    \subfig{intro-to-convex-optimization/images/support-vector-machine.PNG}{.295}
\end{figures}
