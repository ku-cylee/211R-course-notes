\section{Convex Optimization Algorithms}

\subsection{Unconstrained Optimization}

\subsubsection*{Unconstrained Problem}
$$ \begin{aligned}
    \mathrm{minimize}~~&~~f(x)
\end{aligned} $$
\begin{itemize}
    \item Problem assumptions
    \begin{itemize}
        \item $f$: convex, twice differentiable
        \item optimal value $p^\ast = \inf_x f(x)$: Obtained, finite
    \end{itemize}
    \item $\nabla f(x^\ast) = 0$인 점을 찾는 iterative method로 해석될 수 있음
    \item Algorithm assumptions
    \begin{itemize}
        \item Initial point: $x^{(0)}\in\mathcal{D}(f)$
        \item Sublevel set $S = \{x|f(x)\leq f(x^{(0)})\}$: closed
    \end{itemize}
\end{itemize}

\subsubsection*{Strong Convexity and Implications}
\begin{itemize}
    \item $\forall~x\in S$, $\exists~m>0$ s.t. $\nabla^2 f(x)\succeq mI$ $\Rightarrow$ $f$: strongly convex on $S$
    \item Stopping criterion: $f(x)-p^\ast\leq\frac{1}{2m}\Vert\nabla f(x)\Vert_2^2$
    \begin{itemize}
        \item Taylor's Theorem: $f(y)=f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}(y-x)^T\nabla^2f(c)(y-x)$
        \item Strict convexity: $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac{m}{2}\Vert y-x\Vert_2^2$
        \item Minimization of $f(y)$ over $y$: $0=\nabla f(y)=\nabla f(x)+m(y-x)$ $\Rightarrow$ $y=x-\frac{1}{m}\nabla f(x)$
        \item Substituting back: $f(y)\geq f(x)-\nabla f(x)^T\left(\frac{1}{m}\nabla f(x)\right)+\frac{m}{2}\left(\frac{1}{m^2}\Vert\nabla f(x)\Vert_2^2\right)=f(x)-\frac{1}{2m}\Vert\nabla f(x)\Vert_2^2$
        \item Stopping criterion: $f(x)-p^\ast\leq\frac{1}{2m}\Vert\nabla f(x)\Vert_2^2$
        \item Limitations: Deciding $m$ is not known in general
    \end{itemize}
\end{itemize}

\subsubsection*{Descent Methods}
$$ x^{(k+1)}=x^{(k)}+t^{(k)}\Delta x^{(k)}~~~~f(x^{(k+1)})<f(x^{(k)}) $$
\begin{itemize}
    \item $\Delta x$: step, search direction
    \item $t>0$: step size, step length
    \item Convexity $\Rightarrow$ $f(x^{(k+1)})<f(x^{(k)})$ $\Leftrightarrow$ $\nabla f(x^{(k)})^T\Delta x^{(k)}<0$
    \item General Descent Method
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Determine a descent direction $\Delta x$
            \item \textit{Line search} Choose a step size $t>0$
            \item \textit{Update} $x:=x+t\Delta x$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
\end{itemize}

\subsubsection*{Gradient Descent (GD) Method}
\begin{itemize}
    \item $\Delta x = -\nabla f(x)$
    \item Exact line search
        $$t = \arg\min_{t>0}f(x+t\Delta x) = \arg\min_{t>0}f(x-t\nabla f(x))$$
    \begin{itemize}
        \item Step을 줄일 수는 있지만 복잡함
    \end{itemize}
    \item Backtracking line search
    \begin{itemize}
        \item Parameters $\alpha\in\left(0,\frac{1}{2}\right)$, $\beta\in(0,1)$
        \item Starting at $t=1$, repeat $t=\beta t$ until
            $$ f(x+t\Delta x) < f(x)+\alpha t\nabla f(x)^T\Delta x $$
        \item Guarantees $f(x^{(k)})<f(x^{(k+1)})$ since $\nabla f(x)^T\Delta x<0$
    \end{itemize}
    \item Gradient Descent Method: General descent method with $\Delta x = -\nabla f(x)$
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, tolerance $\epsilon>0$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Determine a descent direction $\Delta x=-\nabla f(x)$
            \item \textit{Line search} Choose a step size $t>0$ via exact/backtracking method
            \item \textit{Update} $x:=x+t\Delta x$
        \end{enumerate}
        \item[until] stopping criterion is satisfied; usually $\Vert\nabla f(x)\Vert_2\leq\epsilon$
    \end{description}
    \item Pros of GD
    \begin{itemize}
        \item Convergence result: $f(x^{(k)})-p^\ast\leq c^k\left(f(x^{(0)})-p^\ast\right)$, $c\in(0,1)$
        \item 수렴함이 보장됨, exponential 수렴이므로 매우 빠름 ($c\rightarrow 1$이면 느림)
        \item (대체로) 간단하고 효과적인 알고리즘
    \end{itemize}
    \item Cons of GD: Myopic algorithm $\Rightarrow$ 문제에 따라 매우 느릴 수 있음
\end{itemize}

\subsubsection*{Steepest Descent Method}
\begin{itemize}
    \item Normalized steepest Descent Direction
        $$ \Delta x_\mathrm{nsd}=\arg\min_{v:\Vert v\Vert\leq 1} \left(\nabla f(x)^T v\right) $$
    \begin{itemize}
        \item Interpretation: for $v\ll 1$, $f(x+v)\approx f(x)+\nabla f(x)^T v$
        \item $\Delta x_{\mathrm{nsd}}$: most negative directional derivative
    \end{itemize}
    \item (Unnormalized) Steepest Descent Direction
        $$ \Delta x_\mathrm{sd}=\Vert\nabla f(x)\Vert_\ast\Delta x_\mathrm{nsd} $$
    \begin{itemize}
        \item Interpretation: $\nabla f(x)^Tv\leq\Vert\nabla f(x)^T\Vert_\ast\Vert v\Vert\leq\Vert\nabla f(x)\Vert_\ast$
        \item General form of GD. GD if $\Vert v\Vert_2\leq 1$
    \end{itemize}
    \item Examples
    \begin{itemize}
        \item 2-norm: $\Delta x_\mathrm{sd} = -\nabla f(x)$
        \item Quadratic norm $\Vert x\Vert_P=(x^TPx)^{1/2}$ ($P\in S_{++}^n$): $\Delta x_\mathrm{sd}=-P^{-1}\nabla f(x)$
    \end{itemize}
\end{itemize}

\subsubsection*{Newton's Method}
\begin{itemize}
    \item Newton Step
        $$ \Delta x_\mathrm{nt}=-\nabla^2f(x)^{-1}\nabla f(x) $$
    \begin{itemize}
        \item $\Delta x_\mathrm{nt}$ minimizes second-order approx. over $v$: $f(x+v)=f(x)+\nabla f(x)^Tv+\frac{1}{2}v^T\nabla^2f(x)v$
        \item Steepest descent direction at $x$ in local Hessian norm $\Vert u\Vert_{\nabla^2 f(x)}=\left(u^T\nabla^2 f(x)u\right)^{1/2}$
    \end{itemize}
    \item Newton decrement: Newton step 방향 벡터에 대한 qudratic norm
        $$ \lambda(x) = \left(\nabla f(x)^T\nabla^2f(x)^{-1}\nabla f(x)\right)^{1/2} $$
    \begin{itemize}
        \item $\lambda(x)=\Vert\Delta x_\mathrm{nt}\Vert_{\nabla^2f}=(\Delta x_\mathrm{nt}^T~\nabla^2f~\Delta x_\mathrm{nt})^{1/2}=\left(((\nabla^2 f)^{-1}\nabla f)^T\nabla^2 f((\nabla^2 f)^{-1}\nabla f)\right)^{1/2}$
        \item $\hat{f}$: quadratic approx $\Rightarrow$ $f(x)-p^\ast\approx f(x)-\inf_y\hat{f}(y)=\frac{1}{2}\lambda^2$
        \item Newton direction으로의 방향도 미분: $\nabla f(x)^T\Delta x_\mathrm{nt}=-\lambda^2$
    \end{itemize}
    \item Algorithm
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, tolerance $\epsilon>0$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Compute Newton step and decrement: $\Delta x_\mathrm{nt}=-\nabla^2f(x)^{-1}\nabla f(x)$, $\lambda^2=\Delta x_\mathrm{nt}^T~\nabla^2f(x)~\Delta x_\mathrm{nt}$
            \item \textit{Stopping criterion}: quit if $\frac{1}{2}\lambda^2<\epsilon$
            \item \textit{Line search} Choose a step size $t>0$ via backtracking line search
            \item \textit{Update} $x:=x+t\Delta x_\mathrm{nt}$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
    \item Phases of Newton's Method
    \begin{itemize}
        \item Damped Newton Phase: slow convergence; bottleneck ($\Vert\nabla f(x)\Vert_2\geq\eta$)
        \begin{itemize}
            \item 대부분 step은 backtracking 필요, 함수값이 적어도 $\gamma$만큼 감소
            \item Optimal value: finite $\Rightarrow$ 적어도 $\frac{f(x^{(0)})-p^\ast}{\gamma}$회 이내에 종료
        \end{itemize}
        \item Quadratically Convergent Phase: fast convergence ($\Vert\nabla f(x)\Vert_2<\eta$)
        \begin{itemize}
            \item 모든 step의 $t=1$, $\Vert\nabla f(x)\Vert_2$ converges to 0 quadratically
            \item 적어도 $\log_2\log_2\left(\frac{\epsilon_0}{\epsilon}\right)$회 이내에 종료; Almost constant
        \end{itemize}
    \end{itemize}
    \item $f(x)-p^\ast\leq\epsilon$까지의 반복 횟수는 최대 $\frac{f(x^{(0)})-p^\ast}{\gamma}+\log_2\log_2\left(\frac{\epsilon_0}{\epsilon}\right)$회
    \item High cost, but high convergence speed $\Rightarrow$ tradeoff
\end{itemize}

\subsection{Equality Constrained Optimization}

\subsubsection*{Equality Constrained Problem}
$$ \begin{aligned}
    \mathrm{maximize}~~&~~f(x) \\
    \mathrm{subject~to}~~&~~Ax=b
\end{aligned} $$
\begin{itemize}
    \item Problem assumptions
    \begin{itemize}
        \item $f$: convex, twice differentiable
        \item $A\in\mathbb{R}^{p\times n}$, $\mathrm{rank}A=p$, i.e. $A$: fat matrix
        \item optimal value $p^\ast = \inf_x f(x)$: Obtained, finite
    \end{itemize}
    \newpage
    \item Optimality conditions: $x^\ast$: optimal $\Leftrightarrow$ $\exists~\nu^\ast$ s.t.
    \begin{itemize}
        \item KKT 1: $Ax^\ast=b$
        \item KKT 4: $\nabla f(x^\ast)+A^T\nu=0$
    \end{itemize}
    \item Objective: residual(error) functions $\rightarrow~0$
    \begin{itemize}
        \item Primal residual: $r_p(x,\nu)=Ax-b$
        \item Dual residual: $r_d(x,\nu)=\nabla f(x)+A^T\nu$
    \end{itemize}
\end{itemize}

\subsubsection*{Eliminating Equality Constraints}
$$ \begin{aligned}
    \mathrm{minimize}~~&~~f(x) \\
    \mathrm{subject~to}~~&~~Ax=b
\end{aligned}~~~\Leftrightarrow~~~\begin{aligned}
    \mathrm{minimize}~~&~~\tilde{f}(z)=f(Fz+\hat{x}) \\ {}
\end{aligned} $$
\begin{itemize}
    \item Columns of $F\in\mathbb{R}^{n\times(n-p)}$ form basis of $\mathcal{N}(A)$; $\mathrm{rank}F = n-p$, $AF=0$
    \item $\hat{x}$: any particular solution of $Ax=b$
    \item An unconstrained convex problem over variable $z\in\mathbb{R}^{n\times(n-p)}$
    \item Optimality condition
    \begin{itemize}
        \item $x^\ast=Fz^\ast+\hat{x}$
        \item KKT 4: $0=\nabla f(x^\ast)+A^T\nu^\ast$ $\Rightarrow$ $AA^T\nu^\ast=-A\nabla f(x^\ast)$ $\Rightarrow$ $\nu^\ast=-(AA^T)^{-1}A\nabla f(x^\ast)$
    \end{itemize}
    \item Gradient Descent
    \begin{itemize}
        \item GD step $\Delta z=-F^T\nabla f(Fz+\hat{x})=-F^Tg$
        \item Orthogonal projection to the subspace $\mathcal{V}=\mathrm{span}(f_1,f_2,\cdots,f_n)$
        \item $\Delta x$: closest to true gradient $-g$ $\Rightarrow$ projected gradient method
    \end{itemize}
    \item Newton's Method
    \begin{itemize}
        \item $x^{(k)}=Fz^{(k)}+\hat{x}$
        \item Convergence가 유지됨 (Affine invariance)
    \end{itemize}
\end{itemize}

\subsubsection*{Newton's Method}
\begin{itemize}
    \item Newton step
        $$ \begin{mtx}{cc}\nabla^2f(x)&A^T\\A&0\end{mtx}\begin{mtx}{c}\Delta x_\mathrm{nt}\\w\end{mtx}=\begin{mtx}{c}\nabla f(x)\\0\end{mtx} $$
    \begin{itemize}
        \item Coefficient matrix is called KKT matrix
        \item Interpretation
        \begin{itemize}
            \item $v=\Delta x_\mathrm{nt}$ solves 2nd-order approx. $\hat{f}(x+v)=f(x)+\nabla f(x)^Tv+\frac{1}{2}v^T\nabla^2 f(x)v$ s.t. $A(x+v)=b$
            \item $\Delta x_\mathrm{nt}$ satisfies KKT cond 4: $\nabla f(x+v)+A^Tw\approx\nabla f(x)+\nabla^2 f(x)v+A^Tw=\nabla f(x)+(-\nabla f(x))=0$
        \end{itemize}
    \end{itemize}
    \item Newton decrement
        $$ \lambda(x)=\left(-\nabla f(x)^T\Delta x_\mathrm{nt}\right)^{1/2} $$
    \begin{itemize}
        \item KKT cond 4: $\nabla f+\nabla^2fv+A^Tw=0$
        \item $0=v^T\nabla f+v^T\nabla^2fv+v^TA^Tw=v^T\nabla f+v^T\nabla^2fv+(Av)^Tw=v^T\nabla f+v^T\nabla^2fv$ $\Rightarrow$ $v^T\nabla^2fv=-\nabla f^Tv$
        \item $\lambda=\left(\nabla f^T\nabla^2f^{-1}\nabla f\right)^{1/2}=\left(-\nabla f^Tv\right)^{1/2}$
    \end{itemize}
    \newpage
    \item Algorithm
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$ with $Ax=b$, tolerance $\epsilon>0$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Compute Newton step $\Delta x_\mathrm{nt}$ and decrement $\lambda$
            \item \textit{Stopping criterion}: quit if $\frac{1}{2}\lambda^2<\epsilon$
            \item \textit{Line search} Choose a step size $t>0$ via backtracking line search
            \item \textit{Update} $x:=x+t\Delta x_\mathrm{nt}$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
\end{itemize}

\subsubsection*{Solving KKT Matrix}
$$ K\begin{mtx}{c}v\\w\end{mtx}=\begin{mtx}{cc}H&A^T\\A&0\end{mtx}\begin{mtx}{c}v\\w\end{mtx}=-\begin{mtx}{c}g\\h\end{mtx} $$
\begin{itemize}
    \item $K$: KKT matrix
    \item KKT: nonsingular $\Leftrightarrow$ $\nexists~x\neq 0$ s.t. $Hx=0$, $Ax=0$ $\Leftrightarrow$ $H\succ 0$
    \begin{itemize}
        \item $x\neq 0$, $Ax=0$ $\Rightarrow$ $x\in\mathcal{N}(A)$ $\Rightarrow$ $Hx\neq 0$ $\Rightarrow$ $x^THx\neq 0$ $\Rightarrow$ $x^THx>0$ $\Rightarrow$ $H\succ 0$
        \item $H+A^TA>0$ also holds
        \item $\therefore$ $H\succ 0$ $\Rightarrow$ KKT: nonsingular
    \end{itemize}
    \item Problem is convex optimization problem $\Rightarrow$ $H=\nabla^2 f(x)\succeq 0$
    \item If $H$: nonsingular, i.e. $H=\nabla^2 f(x)\succ 0$
    \begin{itemize}
        \item $Hv+A^Tw=-g$ $\Rightarrow$ $v=-H^{-1}(g+A^Tw)$
        \item $-h=Av=A\left(-H^{-1}(g+A^Tw)\right)=-AH^{-1}(g+A^Tw)$ $\Rightarrow$ $AH^{-1}A^Tw=h-AH^{-1}g$
        \item $w=\left(AH^{-1}A^T\right)^{-1}\left(h-AH^{-1}g\right)$, $v=-H^{-1}\left(g+A^Tw\right)$
    \end{itemize}
    \item If $H$: singular, i.e. $H=\nabla^2 f(x)=0$
    \begin{itemize}
        \item $Hv+A^Tw=-g$ $\Rightarrow$ $Hv+A^TAw+A^Tw=-g-A^Th$ ($\because~Aw=-h$)
        \item $H+A^TA\succ 0$ $\Rightarrow$ Rewrite KKT equation, and apply elimination
        $$ \begin{mtx}{cc}H+A^TA&A^T\\A&0\end{mtx}\begin{mtx}{c}v\\w\end{mtx}=-\begin{mtx}{c}g+A^Th\\h\end{mtx} $$
    \end{itemize}
\end{itemize}
