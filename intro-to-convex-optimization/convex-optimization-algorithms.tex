\section{Convex Optimization Algorithms}

\subsection{Unconstrained Optimization}

\subsubsection*{Unconstrained Problem}
$$ \begin{aligned}
    \mathrm{minimize}~~&~~f(x)
\end{aligned} $$
\begin{itemize}
    \item Problem assumptions
    \begin{itemize}
        \item $f$: convex, twice differentiable
        \item optimal value $p^\ast = \inf_x f(x)$: Obtained, finite
    \end{itemize}
    \item $\nabla f(x^\ast) = 0$인 점을 찾는 iterative method로 해석될 수 있음
    \item Algorithm assumptions
    \begin{itemize}
        \item Initial point: $x^{(0)}\in\mathcal{D}(f)$
        \item Sublevel set $S = \{x|f(x)\leq f(x^{(0)})\}$: closed
    \end{itemize}
\end{itemize}

\subsubsection*{Strong Convexity and Implications}
\begin{itemize}
    \item $\forall~x\in S$, $\exists~m>0$ s.t. $\nabla^2 f(x)\succeq mI$ $\Rightarrow$ $f$: strongly convex on $S$
    \item Stopping criterion: $f(x)-p^\ast\leq\frac{1}{2m}\Vert\nabla f(x)\Vert_2^2$
    \begin{itemize}
        \item Taylor's Theorem: $f(y)=f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}(y-x)^T\nabla^2f(c)(y-x)$
        \item Strict convexity: $f(y)\geq f(x)+\nabla f(x)^T(y-x)+\frac{m}{2}\Vert y-x\Vert_2^2$
        \item Minimization of $f(y)$ over $y$: $0=\nabla f(y)=\nabla f(x)+m(y-x)$ $\Rightarrow$ $y=x-\frac{1}{m}\nabla f(x)$
        \item Substituting back: $f(y)\geq f(x)-\nabla f(x)^T\left(\frac{1}{m}\nabla f(x)\right)+\frac{m}{2}\left(\frac{1}{m^2}\Vert\nabla f(x)\Vert_2^2\right)=f(x)-\frac{1}{2m}\Vert\nabla f(x)\Vert_2^2$
        \item Stopping criterion: $f(x)-p^\ast\leq\frac{1}{2m}\Vert\nabla f(x)\Vert_2^2$
        \item Limitations: Deciding $m$ is not known in general
    \end{itemize}
\end{itemize}

\subsubsection*{Descent Methods}
$$ x^{(k+1)}=x^{(k)}+t^{(k)}\Delta x^{(k)}~~~~f(x^{(k+1)})<f(x^{(k)}) $$
\begin{itemize}
    \item $\Delta x$: step, search direction
    \item $t>0$: step size, step length
    \item Convexity $\Rightarrow$ $f(x^{(k+1)})<f(x^{(k)})$ $\Leftrightarrow$ $\nabla f(x^{(k)})^T\Delta x^{(k)}<0$
    \item General Algorithm
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Determine a descent direction $\Delta x$
            \item \textit{Line search} Choose a step size $t>0$
            \item \textit{Update} $x:=x+t\Delta x$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
\end{itemize}

\subsubsection*{Gradient Descent (GD) Method}
\begin{itemize}
    \item $\Delta x = -\nabla f(x)$
    \item Exact line search
        $$t = \arg\min_{t>0}f(x+t\Delta x) = \arg\min_{t>0}f(x-t\nabla f(x))$$
    \begin{itemize}
        \item Step을 줄일 수는 있지만 복잡함
    \end{itemize}
    \item Backtracking line search
    \begin{itemize}
        \item Parameters $\alpha\in\left(0,\frac{1}{2}\right)$, $\beta\in(0,1)$
        \item Starting at $t=1$, repeat $t=\beta t$ until
            $$ f(x+t\Delta x) < f(x)+\alpha t\nabla f(x)^T\Delta x $$
        \item Guarantees $f(x^{(k)})<f(x^{(k+1)})$ since $\nabla f(x)^T\Delta x<0$
    \end{itemize}
    \item Algorithm: General descent method with $\Delta x = -\nabla f(x)$
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, tolerance $\epsilon>0$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Determine a descent direction $\Delta x=-\nabla f(x)$
            \item \textit{Line search} Choose a step size $t>0$ via exact/backtracking method
            \item \textit{Update} $x:=x+t\Delta x$
        \end{enumerate}
        \item[until] stopping criterion is satisfied; usually $\Vert\nabla f(x)\Vert_2\leq\epsilon$
    \end{description}
    \item Pros of GD
    \begin{itemize}
        \item Convergence result: $f(x^{(k)})-p^\ast\leq c^k\left(f(x^{(0)})-p^\ast\right)$, $c\in(0,1)$
        \item 수렴함이 보장됨, exponential 수렴이므로 매우 빠름 ($c\rightarrow 1$이면 느림)
        \item (대체로) 간단하고 효과적인 알고리즘
    \end{itemize}
    \item Cons of GD: Myopic algorithm $\Rightarrow$ 문제에 따라 매우 느릴 수 있음
\end{itemize}

\subsubsection*{Steepest Descent Method}
\begin{itemize}
    \item Normalized steepest Descent Direction
        $$ \Delta x_\mathrm{nsd}=\arg\min_{v:\Vert v\Vert\leq 1} \left\{\nabla f(x)^T v\right\} $$
    \begin{itemize}
        \item Interpretation: for a very small $v$, $f(x+v)\approx f(x)+\nabla f(x)^T v$
        \item $\Delta x_{\mathrm{nsd}}$: most negative directional derivative
    \end{itemize}
    \item (Unnormalized) Steepest Descent Direction
        $$ \Delta x_\mathrm{sd}=\Vert\nabla f(x)\Vert_\ast\Delta x_\mathrm{nsd} $$
    \begin{itemize}
        \item Interpretation: $\nabla f(x)^Tv\leq\Vert\nabla f(x)\Vert_\ast\Vert v\Vert\leq\Vert\nabla f(x)\Vert_\ast$
        \item General form of GD. GD if $\Vert v\Vert_2\leq 1$
    \end{itemize}
    \item Examples
    \begin{itemize}
        \item 2-norm: $\Delta x_\mathrm{sd} = -\nabla f(x)$
        \item Quadratic norm $\Vert x\Vert_P=(x^TPx)^{1/2}$ ($P\in S_{++}^n$): $\Delta x_\mathrm{sd}=-P^{-1}\nabla f(x)$
    \end{itemize}
\end{itemize}

\subsubsection*{Newton's Method}
\begin{itemize}
    \item Newton Step
        $$ \Delta x_\mathrm{nt}=-\nabla^2f(x)^{-1}\nabla f(x) $$
    \begin{itemize}
        \item $\Delta x_\mathrm{nt}$ minimizes second-order approx. over $v$: $f(x+v)=f(x)+\nabla f(x)^Tv+\frac{1}{2}v^T\nabla^2f(x)v$
        \item Steepest descent direction at $x$ in local Hessian norm $\Vert u\Vert_{\nabla^2 f(x)}=\left(u^T\nabla^2 f(x)u\right)^{1/2}$
    \end{itemize}
    \item Newton decrement: Newton step 방향 벡터에 대한 qudratic norm
        $$ \lambda(x) = \left(\nabla f(x)^T\nabla^2f(x)^{-1}\nabla f(x)\right)^{1/2} $$
    \begin{itemize}
        \item $\lambda(x)=\Vert\Delta x_\mathrm{nt}\Vert_{\nabla^2f}=(\Delta x_\mathrm{nt}^T~\nabla^2f~\Delta x_\mathrm{nt})^{1/2}=\left(((\nabla^2 f)^{-1}\nabla f)^T\nabla^2 f((\nabla^2 f)^{-1}\nabla f)\right)^{1/2}$
        \item $\hat{f}$: quadratic approx $\Rightarrow$ $f(x)-p^\ast\approx f(x)-\inf_y\hat{f}(y)=\frac{1}{2}\lambda^2$
        \item Newton direction으로의 방향도 미분: $\nabla f(x)^T\Delta x_\mathrm{nt}=-\lambda^2$
    \end{itemize}
    \item Algorithm
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, tolerance $\epsilon>0$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Compute Newton step and decrement: $\Delta x_\mathrm{nt}=-\nabla^2f(x)^{-1}\nabla f(x)$, $\lambda^2=\Delta x_\mathrm{nt}^T~\nabla^2f(x)~\Delta x_\mathrm{nt}$
            \item \textit{Stopping criterion}: quit if $\frac{1}{2}\lambda^2<\epsilon$
            \item \textit{Line search} Choose a step size $t>0$ via backtracking line search
            \item \textit{Update} $x:=x+t\Delta x_\mathrm{nt}$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
    \item Phases of Newton's Method
    \begin{itemize}
        \item Damped Newton Phase: slow convergence; bottleneck ($\Vert\nabla f(x)\Vert_2\geq\eta$)
        \begin{itemize}
            \item 대부분 step은 backtracking 필요, 함수값이 적어도 $\gamma$만큼 감소
            \item Optimal value: finite $\Rightarrow$ 적어도 $\frac{f(x^{(0)})-p^\ast}{\gamma}$회 이내에 종료
        \end{itemize}
        \item Quadratically Convergent Phase: fast convergence ($\Vert\nabla f(x)\Vert_2<\eta$)
        \begin{itemize}
            \item 모든 step의 $t=1$, $\Vert\nabla f(x)\Vert_2$ converges to 0 quadratically
            \item 적어도 $\log_2\log_2\frac{\epsilon_0}{\epsilon}$회 이내에 종료; Almost constant
        \end{itemize}
    \end{itemize}
    \item $f(x)-p^\ast\leq\epsilon$까지의 반복 횟수는 최대 $\frac{f(x^{(0)})-p^\ast}{\gamma}+\log_2\log_2\frac{\epsilon_0}{\epsilon}$회
    \item High cost, but high convergence speed $\Rightarrow$ tradeoff
\end{itemize}

\subsection{Equality Constrained Optimization}

\subsubsection*{Equality Constrained Problem}
$$ \begin{aligned}
    \mathrm{minimize}~~&~~f(x) \\
    \mathrm{subject~to}~~&~~Ax=b
\end{aligned} $$
\begin{itemize}
    \item Problem assumptions
    \begin{itemize}
        \item $f$: convex, twice differentiable
        \item $A\in\mathbb{R}^{p\times n}$, $\mathrm{rank}A=p$, i.e. $A$: fat matrix
        \item Optimal value $p^\ast = \inf_x f(x)$: Obtained, finite
    \end{itemize}
    \newpage
    \item Optimality conditions: $x^\ast$: optimal $\Leftrightarrow$ $\exists~\nu^\ast$ s.t.
    \begin{itemize}
        \item KKT 1: $Ax^\ast=b$
        \item KKT 4: $\nabla f(x^\ast)+A^T\nu^\ast=0$
    \end{itemize}
    \item Objective: residual(error) functions $\rightarrow~0$
    \begin{itemize}
        \item Primal residual: $r_p(x,\nu)=Ax-b$
        \item Dual residual: $r_d(x,\nu)=\nabla f(x)+A^T\nu$
    \end{itemize}
\end{itemize}

\subsubsection*{Eliminating Equality Constraints}
$$ \begin{aligned}
    \mathrm{minimize}~~&~~f(x) \\
    \mathrm{subject~to}~~&~~Ax=b
\end{aligned}~~~\Leftrightarrow~~~\begin{aligned}
    \mathrm{minimize}~~&~~\tilde{f}(z)=f(Fz+\hat{x}) \\ {}
\end{aligned} $$
\begin{itemize}
    \item Columns of $F\in\mathbb{R}^{n\times(n-p)}$ form basis of $\mathcal{N}(A)$; $\mathrm{rank}F = n-p$, $AF=O$
    \item $\hat{x}$: any particular solution of $Ax=b$
    \item An unconstrained convex problem over variable $z\in\mathbb{R}^{n\times(n-p)}$
    \item Optimality condition
    \begin{itemize}
        \item $x^\ast=Fz^\ast+\hat{x}$
        \item KKT 4: $0=\nabla f(x^\ast)+A^T\nu^\ast$ $\Rightarrow$ $AA^T\nu^\ast=-A\nabla f(x^\ast)$ $\Rightarrow$ $\nu^\ast=-(AA^T)^{-1}A\nabla f(x^\ast)$
    \end{itemize}
    \item Gradient Descent
    \begin{itemize}
        \item GD step $\Delta z=-F^T\nabla f(Fz+\hat{x})=-F^Tg$
        \item Orthogonal projection to the subspace $\mathcal{V}=\mathrm{span}(f_1,f_2,\cdots,f_n)$
        \item $\Delta x$: closest to true gradient $-g$ $\Rightarrow$ projected gradient method
    \end{itemize}
    \item Newton's Method
    \begin{itemize}
        \item $x^{(k)}=Fz^{(k)}+\hat{x}$
        \item Convergence가 유지됨 (Affine invariance)
    \end{itemize}
\end{itemize}

\subsubsection*{Newton's Method}
\begin{itemize}
    \item Newton step
        $$ \begin{mtx}{cc}\nabla^2f(x)&A^T\\A&0\end{mtx}\begin{mtx}{c}\Delta x_\mathrm{nt}\\w\end{mtx}=\begin{mtx}{c}-\nabla f(x)\\0\end{mtx} $$
    \begin{itemize}
        \item Coefficient matrix is called KKT matrix
        \item Interpretation
        \begin{itemize}
            \item $v=\Delta x_\mathrm{nt}$ solves 2nd-order approx. $\hat{f}(x+v)=f(x)+\nabla f(x)^Tv+\frac{1}{2}v^T\nabla^2 f(x)v$ s.t. $A(x+v)=b$
            \item $\Delta x_\mathrm{nt}$ satisfies KKT cond 4: $\nabla f(x+v)+A^Tw\approx\nabla f(x)+\nabla^2 f(x)v+A^Tw=\nabla f(x)+(-\nabla f(x))=0$
        \end{itemize}
        \item $x+\Delta x_\mathrm{nt}$, $w$: a good estimate of $x^\ast$ and $\nu^\ast$ when $f$ is nearly quadratic
    \end{itemize}
    \item Newton decrement
        $$ \lambda(x)=\left(-\nabla f(x)^T\Delta x_\mathrm{nt}\right)^{1/2} $$
    \begin{itemize}
        \item KKT cond 4: $\nabla f+\nabla^2fv+A^Tw=0$
        \item $0=v^T\nabla f+v^T\nabla^2fv+v^TA^Tw=v^T\nabla f+v^T\nabla^2fv+(Av)^Tw=v^T\nabla f+v^T\nabla^2fv$ $\Rightarrow$ $v^T\nabla^2fv=-\nabla f^Tv$
        \item $\lambda=\left(\nabla f^T\nabla^2f^{-1}\nabla f\right)^{1/2}=\left(-\nabla f^Tv\right)^{1/2}$
    \end{itemize}
    \newpage
    \item Algorithm
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$ with $Ax=b$, tolerance $\epsilon>0$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Compute Newton step $\Delta x_\mathrm{nt}$ and decrement $\lambda$
            \item \textit{Stopping criterion}: quit if $\frac{1}{2}\lambda^2<\epsilon$
            \item \textit{Line search} Choose a step size $t>0$ via backtracking line search
            \item \textit{Update} $x:=x+t\Delta x_\mathrm{nt}$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
\end{itemize}

\subsubsection*{Solving KKT Matrix}
$$ K\begin{mtx}{c}v\\w\end{mtx}=\begin{mtx}{cc}H&A^T\\A&0\end{mtx}\begin{mtx}{c}v\\w\end{mtx}=-\begin{mtx}{c}g\\h\end{mtx} $$
\begin{itemize}
    \item KKT matrix ($K$): nonsingular $\Leftrightarrow$ $\nexists~x\neq 0$ s.t. $Hx=0$, $Ax=0$ $\Leftrightarrow$ $H\succ 0$
    \begin{itemize}
        \item $x\neq 0$, $Ax=0$ $\Rightarrow$ $x\in\mathcal{N}(A)$ $\Rightarrow$ $Hx\neq 0$ $\Rightarrow$ $x^THx\neq 0$ $\Rightarrow$ $x^THx>0$ $\Rightarrow$ $H\succ 0$
        \item $H+A^TA>0$ also holds
        \item $\therefore$ $H\succ 0$ $\Rightarrow$ KKT: nonsingular
    \end{itemize}
    \item Problem is convex optimization problem $\Rightarrow$ $H=\nabla^2 f(x)\succeq 0$
    \item If $H$: nonsingular, i.e. $H=\nabla^2 f(x)\succ 0$
    \begin{itemize}
        \item $Hv+A^Tw=-g$ $\Rightarrow$ $v=-H^{-1}(g+A^Tw)$
        \item $-h=Av=A\left(-H^{-1}(g+A^Tw)\right)=-AH^{-1}(g+A^Tw)$ $\Rightarrow$ $AH^{-1}A^Tw=h-AH^{-1}g$
        \item $w=\left(AH^{-1}A^T\right)^{-1}\left(h-AH^{-1}g\right)$, $v=-H^{-1}\left(g+A^Tw\right)$
    \end{itemize}
    \item If $H$: singular, i.e. $H=\nabla^2 f(x)=0$
    \begin{itemize}
        \item $Hv+A^Tw=-g$ $\Rightarrow$ $Hv+A^TAw+A^Tw=-g-A^Th$ ($\because~Aw=-h$)
        \item $H+A^TA\succ 0$ $\Rightarrow$ Rewrite KKT equation, and apply elimination
        $$ \begin{mtx}{cc}H+A^TA&A^T\\A&0\end{mtx}\begin{mtx}{c}v\\w\end{mtx}=-\begin{mtx}{c}g+A^Th\\h\end{mtx} $$
    \end{itemize}
\end{itemize}

\subsection{Inequality Constrained Optimization}

\subsubsection*{Inequality Constrained Problem}
$$ \begin{aligned}
    \mathrm{minimize}~~&~~f(x) \\
    \mathrm{subject~to}~~&~~f_i(x)\leq 0~~(i=1,2,\cdots,m) \\
        &~~Ax=b
\end{aligned} $$
\begin{itemize}
    \item Problem assumptions
    \begin{itemize}
        \item $f$: convex, twice differentiable
        \item $A\in\mathbb{R}^{p\times n}$, $\mathrm{rank}A=p$, i.e. $A$: fat matrix
        \item optimal value $p^\ast = \inf_x f(x)$: Obtained, finite
        \item Problem is strictly feasible, i.e. $\exists~x\in\mathcal{D}(f_0)$, $f_i(x)<0$, $Ax=b$
    \end{itemize}
\end{itemize}

\subsubsection*{Logarithmic Barrier}
\begin{itemize}
    \item Elminating inequality constraints by introducing irritation function (blue, dashed)
        $$ \begin{aligned}
            \mathrm{minimize}~~&~~f_0(x)+\sum_{i=1}^mI_-(f_i(x)) \\
            \mathrm{subject~to}~~&~~Ax=b
        \end{aligned} $$
    \begin{itemize}
        \item $I_-(x)$: $0$ if $x<0$, $\infty$ if $x\geq 0$
        \item Irritation function is difficult to handle in a mathematical manner
    \end{itemize}
    \item Logarithmic Barrier (red, solid)
        $$ \phi(x)=-\sum_{i=1}^m\log(-f_i(x)),~~~~\mathcal{D}(\phi)=\{x|f_1(x)<0,f_2(x)<0,\cdots,f_m(x)<0\} $$
    \begin{itemize}
        \item $f_i$: convex $\Rightarrow$ $\phi(x)$ is convex
        \item $\nabla\phi(x)=-\sum_{i=1}^{m}\frac{1}{f_i(x)}\nabla f_i(x)$
        \item $\nabla^2\phi(x)=\sum_{i=1}^{m}\left(\frac{1}{f_i(x)^2}\nabla f_i(x)\nabla f_i(x)^T-\frac{1}{f_i(x)}\nabla^2f_i(x)\right)$
    \end{itemize}
    \item Approximation via Logarithmic Barrier
        $$ \begin{aligned}
            \mathrm{minimize}~~&~~tf_0(x)+\phi(x) \\
            \mathrm{subject~to}~~&~~Ax=b
        \end{aligned} $$
    \begin{itemize}
        \item An equality constrained problem
        \item For $t>0$, $-\frac{1}{t}\phi(x)$ is a smooth approximation of $I_-$
        \item Boundary points cannot be a solution; Not a big problem in an engineering problem
        \item Approximation improves as $t\rightarrow\infty$
    \end{itemize}
    \begin{figures}
        \fig{log-barrier.PNG}{.4}
    \end{figures}
\end{itemize}

\subsubsection*{Dual points on Central path}
\begin{itemize}
    \item Central path: $\{x^\ast(t)~|~t>0\}$
    \begin{itemize}
        \item $x^\ast(t)$: Solution of the problem $f(x)=tf_0(x)+\phi(x)$ s.t. $Ax=b$ (Assume $\forall~t>0$, $\exists!~x^\ast(t)$)
    \end{itemize}
    \item $\exists~w$ s.t. $Ax=b$, $t\nabla f_0(x)+\sum_{i=1}^m\frac{1}{-f_i(x)}\nabla f_i(x)+A^Tw = 0$ $\Rightarrow$ $x=x^\ast(t)$
    \item Lagrangian: $L(x,\lambda^\ast(t),\nu^\ast)=f_0(x)+\sum_{i=1}^{m}\lambda_i^\ast(t)f_i(x)+\nu^\ast(t)^T(Ax-b)$
    \item $x^\ast(t)$ minimizes $L$ where $\lambda_i^\ast(t)=-\frac{1}{tf_i(x^\ast(t))}$, $\nu^\ast(t)=\frac{w}{t}$
    \item $p^\ast\geq g(\lambda^\ast,\nu^\ast)=L(x^\ast,\lambda^\ast,\nu^\ast)=f_0(x^\ast)-\sum_{i=1}^m\frac{f_i(x^\ast)}{tf_i(x^\ast)}+\nu^{\ast T}(Ax-b)=f_0(x^\ast)-\frac{m}{t}$
    \item $f_0(x^\ast)-p^\ast\leq\frac{m}{t}\rightarrow 0$ when $t\rightarrow\infty$
\end{itemize}

\subsubsection*{Interpretation via KKT conditions}
\begin{itemize}
    \item Primal constraints: $f_i(x)<0\leq 0$
    \item Dual constraints: $\lambda=-\frac{1}{tf_i(x^\ast)}>0$ ($\because$ primal constraints)
    \item \textit{Approximate} complementary slackness: $\lambda_if_i(x)=-\frac{1}{t}\rightarrow 0$ when $t\rightarrow\infty$
    \item Zero Lagrangian gradient holds
\end{itemize}

\subsubsection*{Barrier Method}
\begin{itemize}
    \item Algorithm
    \begin{description}
        \item[given] a strictly feasible starting point $x$, $t:=t^{(0)}>0$, $\mu>1$ with tolerance $\epsilon>0$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item \textit{Centering step}: Compute $x^\ast$ by minimizing $tf_0+\phi$ s.t. $Ax=b$: Another loop
            \item \textit{Update}: $x=x^\ast(t)$
            \item \textit{Stopping criterion}: quit if $\frac{m}{t}<\epsilon$
            \item \textit{Increase $t$}: $t:=\mu t$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
    \item \textit{Centering} is usually done by Newton's method, starting at current $x$, i.e. $x^\ast$ of prev. step
    \item Choice of $\mu$ $\Rightarrow$ tradeoff
    \begin{itemize}
        \item Large $\mu$: fewer outer iterations, more inner iterations
        \item Small $\mu$: more outer iterations, fewer inner iterations
        \item Typical $\mu=10~-~20$
    \end{itemize}
    \item Number of outer iterations
        $$ \left\lceil\frac{\log\frac{m}{\epsilon t^{(0)}}}{\log\mu}\right\rceil~+~\mathrm{initial~centering~step} $$
\end{itemize}
