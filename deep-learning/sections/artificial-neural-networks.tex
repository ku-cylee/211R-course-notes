\section{Artifical Neural Networks}

\subsection{Neural Networks}

\subsubsection*{Motivation}
\begin{itemize}
    \item Pros and cons of linear classifier
    \begin{itemize}
        \item Pros: Simple $\Rightarrow$ Very expressive
        \item Cons: Not very powerful (e.g. XOR problem)
    \end{itemize}
    \item Human's thinking process is hierachial (low level $\rightarrow$ high level)
    \begin{itemize}
        \item e.g. Book reading: letters $\rightarrow$ words $\rightarrow$ sentences $\rightarrow$ story
    \end{itemize}
    \item Insert non-linear models between linear models $\Rightarrow$ \textbf{Deep Learning}
    \item Non-linear function $f$ is called ``activation function''
    \begin{itemize}
        \item $f$ is elementwise, i.e. $f\left((x_1,x_2,\cdots,x_n)\right)=(f(x_1),f(x_2),\cdots,f(x_n))$
    \end{itemize}
\end{itemize}
\begin{figures}
    \subfig{Nonlinear models}{nonlinear-models.png}{.3808}
    \subfig{Activation function}{activation-function.png}{.4192}
\end{figures}

\subsubsection*{Multilayer Neural Network}
\begin{itemize}
    \item $n$-layer neural network, or $(n-1)$-hidden-layer neural network
    \item 1 input layer $\rightarrow$ $(n-1)$ hidden layer $\rightarrow$ 1 output layer
    \item Scores on hidden layer are called ``intermediate output''
    \item Every hidden \& output layer has non-linear function $f$
    \begin{itemize}
        \item If not, it can be merged to linear model
    \end{itemize}
    \item Fully connected layers: Every layers are connected each other
\end{itemize}
\begin{figures}
    \fig{multilayer-neural-network.png}{.45}
\end{figures}
\clearpage

\subsubsection*{Computing Neural Network}
\begin{itemize}
    \item Consider $k$-th(input) layer has $m$ classes $\Rightarrow$ Scores $x=x_1,x_2,\cdots,x_m$
    \item Consider $(k+1)$-th(output) layer has $n$ classes $\Rightarrow$ Scores $y=y_1,y_2,\cdots,y_n$
    \item Let $w_{ij}$ be the parameter from $i$-th input to $j$-th output, $w_j=(w_{1j},w_{2j},\cdots,w_{mj})$
    \item Let $b_j$ be the bias from input layer to $j$-th output
    \item Let $f$ be the activation function on output layer
    \item Then, value of $j$-th output is
    \begin{equation}
        y_j=f(w_{1j}x_1+w_{2j}x_2+\cdots+w_{ij}x_i+b_j)=f(w_j^Tx+b_j)
    \end{equation}
    \item Let $W=\left(w_1^T,w_2^T,\cdots,w_n^T\right)$ and $b=(b_1,b_2,\cdots,b_n)$, then
    \begin{equation}
        y=f(Wx+b),\text{ i.e. }\begin{mtx}{c}
            y_1 \\ y_2 \\ \vdots \\ y_n
        \end{mtx}=f\left(\begin{mtx}{cccc}
            w_{11} & w_{21} & \cdots & w_{n1}\\
            w_{12} & w_{22} & \cdots & w_{n2}\\
            \vdots & \vdots & \ddots & \vdots\\
            w_{1m} & w_{2m} & \cdots & w_{nm}
        \end{mtx}\begin{mtx}{c}
            x_1 \\ x_2 \\ \vdots \\ x_m
        \end{mtx}\right)
    \end{equation}
    \item For $k$-layer nerual network, $y=f_k(W_kf_{k-1}(W_{k-1}\cdots f_1(W_1x)))$
\end{itemize}
\begin{figures}
    \fig{neural-network-calculation.png}{.25}
\end{figures}

\subsubsection*{Activation Functions}
\begin{itemize}
    \item ReLU (Rectified Linear Unit)
    \begin{equation}
        f(x)=\max\{0,x\}
    \end{equation}
    \begin{itemize}
        \item Filters out $x<0$ $\Rightarrow$ rectifies information
        \item Simple calculation, similar to linear $\Rightarrow$ 표현력과 최적화 모두 좋음 $\Rightarrow$ Good choice for many problems
    \end{itemize}
    \item Sigmoid ($\sigma(x)=\frac{1}{1+e^{-x}}$), Hyper-tangent ($f(x)=\tanh(x)$)
    \item Maxout ($f(x)=\max\{w_1^Tx+b_1,w_2^Tx+b_2\}$)
    \item Leaky ReLU ($f(x)=\max\{\alpha x,x\}~(\alpha>0)$), ELU ($f(x)=x~(x\geq 0),~\alpha(e^x-1)~(x<0)$)
\end{itemize}
\begin{figures}
    \subfig{ReLU}{actfunc-relu.jpg}{.18}
    \subfig{Sigmoid}{actfunc-sigmoid.jpg}{.18}
    \subfig{tanh}{actfunc-tanh.jpg}{.18}
    \subfig{Leaky ReLU}{actfunc-leaky-relu.jpg}{.18}
    \subfig{ELU}{actfunc-elu.jpg}{.18}
\end{figures}

\subsection{Optimizing Loss Functions}

\subsubsection*{Optimization}
\begin{itemize}
    \item Optimization: to minimize or maximize a function $f:\mathbb{R}^n\to\mathbb{R}$
    \item $f(x)$ is minimum at $x=x^\ast$ $\Rightarrow$ $x^\ast$: optimal point, $f(x^\ast)$: optimal value
    \begin{itemize}
        \item $x=x^\ast$ is (global) optimal: $\forall x\in\mathcal{D}(f)$, $f(x^\ast)\leq f(x)$
        \item $x=x^\ast$ is local optimal: $\exists~R>0$ s.t. $x^\ast$ is optimal point for $f$ at $\Vert x-x^\ast\Vert\leq R$
    \end{itemize}
    \item Optimization methods
    \begin{itemize}
        \item If $\exists~f^\prime$, all optimal points $x=x^\ast$ satisfies $\nabla f(x^\ast)=0$
        \item Produce a sequence of points $x^{(k)}$ for $k=1,2,\cdots$ such that $f\left(x^{(k)}\right)\to f\left(x^\ast\right)$
    \end{itemize}
\end{itemize}

\subsubsection*{Gradient}
\begin{itemize}
    \item Gradient is a vector of partial derivatives of function $f:\mathbb{R}^n\rightarrow\mathbb{R}$
    \begin{equation}
        \nabla f(x)=\left(\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2},\cdots,\frac{\partial f(x)}{\partial x_n}\right)
    \end{equation}
    \item Taylor expansion: $f(x + \Delta x) \approx f(x) + \nabla f(x)^T \Delta x$
    \item Example
    \begin{itemize}
        \item $f(x)=c^Tx$ $\Rightarrow$ $\nabla f(x)=c$
        \item $f(x)=x^TQx$ $\Rightarrow$ $\nabla f(x)=2Qx$
    \end{itemize}
    \item Gradient vector $\nabla f$ points in the direction of greatest increase/decrease of $f$
    \begin{itemize}
        \item Directional derivative $\nabla f(x)^T\Delta x$: Rate of change to direction $\Delta x$
        \item For any $\Vert\Delta x\Vert=1$, $f(x)$ increases the most if $\Delta x=\frac{\nabla f(x)}{\Vert\nabla f(x)\Vert}$
        \item For any $\Vert\Delta x\Vert=1$, $f(x)$ decreases the most if $\Delta x=-\frac{\nabla f(x)}{\Vert\nabla f(x)\Vert}$
    \end{itemize}
\end{itemize}

\subsubsection*{Gradient Descent (GD) Method}
\begin{itemize}
    \item Algorithm: General descent method with $\Delta x = -\nabla f(x)$
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, tolerance $\epsilon>0$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Determine a descent direction $\Delta x=-\nabla f(x)$
            \item \textit{Line search} Choose a step size $\alpha>0$
            \item \textit{Update} $x:=x+\alpha\Delta x$
        \end{enumerate}
        \item[until] stopping criterion is satisfied; usually $\Vert\nabla f(x)\Vert_2\leq\epsilon$
    \end{description}
    \item Pros of GD
    \begin{itemize}
        \item Convergence result: $f(x^{(k)})-p^\ast\leq c^k\left(f(x^{(0)})-p^\ast\right)$, $c\in(0,1)$
        \item Convergence is guaranteed and mostly very fast ($\because$ exponential)
        \item Most cases, simple and efficient
    \end{itemize}
    \item Cons of GD
    \begin{itemize}
        \item Myopic algorithm: Can be very slow for some problems ($c\to 1$)
        \item GD may find a local optimal, but not necessarily global optimal
    \end{itemize}
\end{itemize}

\subsubsection*{GD Algorithm for Optimizing Loss Functions}
\begin{itemize}
    \item Objective of deep learning is to optimize loss function $L(W)$
    \item By applying GD algorithm, repeat
    \begin{equation}
        W\leftarrow W-\alpha\nabla L(W)
    \end{equation}
    \item GD may fail to find global optimal
    \begin{itemize}
        \item But, local optimal values of neural networks are similar
        \item Not a big problem at neural networks
    \end{itemize}
    \item Directly computing $\nabla L(W)$ by differentiation is complicated! $\Rightarrow$ Backpropagation
\end{itemize}
