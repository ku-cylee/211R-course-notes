\section{Training Neural Networks}

\subsection{Optimization Algorithms}

\subsubsection*{Stochastic Gradient Descent (SGD)}
\begin{itemize}
    \item Problems of Gradient Descent(GD)
    \begin{itemize}
        \item GD update: $\nabla L(W)=\frac{1}{N}\nabla L_i(f(x_i,W),y_i)$
        \item Large $N$ causes long time for each step
    \end{itemize}
    \item Randomized, simplified version of GD
    \begin{itemize}
        \item Randomly sampled inputs randomize descending direction
        \item We can \textit{escape} local valley with local minimal
    \end{itemize}
    \item Algorithm
    \begin{enumerate}
        \item Select \textbf{one} random data sample: choose $k$ from $1,2,\cdots,N$
        \item Update $W\leftarrow W-\alpha\nabla_WL_k(f(x_k,W),y_k)$
    \end{enumerate}
    \item MSE minimization: $W\leftarrow W-2\gamma\left(\hat{f}(x_k,W)-y_k\right)\nabla_Wf(x_k,W)$
    \item $\nabla L_k$: a noisy, inaccurate estimate of true gradient $\nabla L$
    \begin{itemize}
        \item $\mathbb{E}_k[\nabla L_k(W)]=\nabla L(W)$
        \item an unbiased estimator, i.e. correct direction in long-term
        \item SGD may not converge, but oscillate nearby optimum
    \end{itemize}
    \item Traditional GD is called \textit{batch GD}
    \item Problem of SGD: stepwise computation is fast, but \# of steps is large
\end{itemize}

\subsubsection*{Mini-batch SGD}
\begin{itemize}
    \item Choose a random subset of dataset: Select $M$ samples, and use their mean gradient
    \item Algorithm
    \begin{enumerate}
        \item Select a random subset of data sample: choose $B\in\{1,2,\cdots,N\}$
        \item Update $W\leftarrow W-\gamma\frac{1}{|B|}\sum_{k\in B}\nabla_WL_k(f(x_k,W),y_k)$
    \end{enumerate}
    \item Batch size $M$
    \begin{itemize}
        \item $M$ is typically $2^n$ (32, 64, $\cdots$): Memory access efficiency
        \item $M$ $\uparrow$ $\implies$ Error variance $\downarrow$
        \item $M=1$: SGD, $M=N$: Batch GD
    \end{itemize}
    \item Also unbiased estimator of $\nabla L$ $\implies$ correct direction in long-term
    \item Mini-batch SGD is a sufficiently good solution, practically
\end{itemize}

\subsubsection*{SGD with Momentum and EWMA}
\begin{itemize}
    \item Problems of SGD: Oscillates on valley-like contours, Very noisy
    \item Idea: EWMA(Exponentially Weighted Moving Average)
    \begin{itemize}
        \item Enhance a \textit{long-term} trend(history) of gradient
        \item Suppress \textit{short-term} fluctuations in the gradient
    \end{itemize}
    \clearpage
    \item Update Rule
    \begin{itemize}
        \item Gradient vector $g_t=\nabla L_J(x_t)$ from SGD, noisy version of true gradient
        \item Introduce friction $\alpha\in(0,1)$ and velocity $v_t$, which is EWMA on $g_t$
        \begin{equation}
            v_{t+1}=\alpha v_t+(1-\alpha)g_t
        \end{equation}
        \item Update $x_t$: considered as position
        \begin{equation}
            x_{t+1}=x_t-\gamma v_t
        \end{equation}
    \end{itemize}
    \item Solving for $v_t$
    \begin{equation}\label{eq:dl:ewma-solution}
        v_t=(1-\alpha)\sum_{i=1}^t\alpha^{t-i}g_i
    \end{equation}
    \begin{itemize}
        \item Past values of $g_t$: exponential decay $\implies$ forgotten
        \item $\alpha\to0$: aggresively forget; $\alpha\to1$: more weight on past
        \item Velocity with common(right) direction accumulates, noise direction vanishes
    \end{itemize}
    \item Removing bias
    \begin{itemize}
        \item Weights of $g_t$ should sum up to $1$
        \item From \ref{eq:dl:ewma-solution}, $(1-\alpha)\sum_{i=1}^t\alpha^{t-i}=1-\alpha^t$
        \item Bias-corrected estimate is given
        \begin{equation}
            \hat{v}_t=\frac{v_t}{1-\alpha^t},~x_{t+1}\leftarrow x_t-\hat{v}_t
        \end{equation}
        \item As $t\to\infty$, $\hat{v}_t\to v_t$ $\implies$ optional
    \end{itemize}
    \item Typical friction is $\alpha=0.9$
    \item Momentum SGD works mostly better than vanilla SGD
\end{itemize}

\subsubsection*{RMSProp}
\begin{itemize}
    \item Algorithm
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, friction $\alpha$, learning rate $\gamma$, param. $\varepsilon$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Sample data and compute $g\leftarrow\nabla L_i(x)$
            \item \textit{Accumulate:} $r\leftarrow\alpha r+(1-\alpha)g\odot g$
            \item \textit{Update:} $x\leftarrow x-\frac{\gamma}{\sqrt{r}+\varepsilon}\odot g$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
    \item $\odot$: Elementwise product of vectors
    \item $\varepsilon>0$: prevents numerical instability, i.e. denominator $\to 0$
    \item Adaptive annealing of learning rates
    \begin{itemize}
        \item Large gradient $\to$ decrease l.r.; Small gradient $\to$ increase l.r.
        \item Gradients among variables may vary $\implies$ Different gradients among direction
        \begin{equation}
            r_i\leftarrow\alpha r_i+(1-\alpha)g_i^2,~x_i\leftarrow x_i-\frac{\gamma}{\sqrt{r_i}+\varepsilon}g_i
        \end{equation}
        \item Scaling l.r. of each component w/ weighted RMS(root-mean-square) of past gradients
    \end{itemize}
\end{itemize}

\subsubsection*{Adam}
\begin{itemize}
    \item Combination of momentum and RMSprop
    \item Algorithm
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, learning rate $\gamma$, friction $\alpha,\beta$, param. $\varepsilon$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Sample data and compute $g\leftarrow\nabla L_i(x)$
            \item \textit{1st moment estimate:} $v\leftarrow\alpha v+(1-\alpha)g$
            \item \textit{2nd moment estimate:} $r\leftarrow\beta r+(1-\beta)g\odot g$
            \item \textit{Bias correction:} $\hat{v}\leftarrow\frac{v}{1-\alpha^t}$, $\hat{r}\leftarrow\frac{r}{1-\beta^t}$
            \item \textit{Update:} $x\leftarrow x-\frac{\gamma}{\sqrt{\hat{r}+\varepsilon}}\odot\hat{v}$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
    \item Hyperparameters
    \begin{itemize}
        \item $\alpha$: 1st moment decay rate; typically 0.9
        \item $\beta$: 2nd moment decay rate; typically 0.99
        \item $\gamma$: global learning rate; typically $10^{-3}$
        \item $\varepsilon>0$: numerical stability; typically $10^{-8}$
    \end{itemize}
\end{itemize}

\subsubsection*{Decoupled Regularization}
\begin{itemize}
    \item Idea: L2 Regularization
    \begin{equation}
        x\leftarrow x-\alpha\nabla\left[f(x)+\gamma\frac{\norm{x}_2^2}{2}\right]=(1-\alpha\gamma)x-\alpha\nabla f(x)
    \end{equation}
    \begin{itemize}
        \item Weight decay rate $\alpha\gamma$ and learning rate $\alpha$: Tightly coupled(dependent)
        \item \textit{Decoupling} two parameters shows far better performance
        \item Replace weight decay rate $\alpha\gamma$ to an independent parameter $\lambda$
    \end{itemize}
    \item SGDW: Momentum SGD + Decoupled weight decay
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, learning rate $\gamma$, friction $\alpha$, weight decay $\lambda$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Sample data and compute $g\leftarrow\nabla L_i(x)$
            \item \textit{Accumulate:} $v\leftarrow\alpha v+(1-\alpha)g$
            \item \textit{Update:} $x\leftarrow (1-\lambda)x-\gamma v$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
    \item AdamW: Adam + Decoupled Weight Decay; coupling on gradient $g$ affects $v,r,x$
    \begin{description}
        \item[given] a starting point $x\in\mathcal{D}(f)$, learning rate $\gamma$, friction $\alpha,\beta$, weight decay $\lambda$, param. $\varepsilon$
        \item[repeat] \phantom{}
        \begin{enumerate}
            \item Sample data and compute $g\leftarrow\nabla L_i(x)$
            \item \textit{1st moment estimate:} $v\leftarrow\alpha v+(1-\alpha)g$
            \item \textit{2nd moment estimate:} $r\leftarrow\beta r+(1-\beta)g\odot g$
            \item \textit{Bias correction:} $\hat{v}\leftarrow\frac{v}{1-\alpha^t}$, $\hat{r}\leftarrow\frac{r}{1-\beta^t}$
            \item \textit{Update:} $x\leftarrow (1-\lambda)x-\frac{\gamma}{\sqrt{\hat{r}+\varepsilon}}\odot\hat{v}$
        \end{enumerate}
        \item[until] stopping criterion is satisfied
    \end{description}
\end{itemize}

\subsection{Activation Functions}

\subsubsection*{Sigmoid and Hypertangent}
\begin{equation}
    \sigma(x)=\frac{1}{1+e^{-x}},~\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
\end{equation}
\begin{itemize}
    \item Pros of sigmoid: Saturates ``firing rate'' of a neuron $\implies$ nice interpretation
    \item Cons of sigmoid
    \begin{itemize}
        \item Saturated neurons \textit{kill} the gradients
        \begin{itemize}
            \item $\abs{x}\gg1\implies\frac{d\sigma}{dx}\to0$: Vanishing gradient problem
        \end{itemize}
        \item Output is always positive, Not zero-centered(zero-mean) $\implies$ Limits search direction greatly
        \begin{itemize}
            \item Consider sigmoid layer $\to$ linear layer ($f(x)=Wx+b$) $\implies$ $x>0$
            \item Consider backprop. on linear layer $\frac{\partial L}{\partial W}=\frac{\partial L}{\partial f}\frac{\partial f}{\partial W}=\frac{\partial L}{\partial f}x$
            \item Possible direction of $\frac{\partial L}{\partial W}$ are: All components positive or all negative
            \item Possible signs of search direction: $2^n\to 2$ $\implies$ Greatly limited
        \end{itemize}
        \item Exponential functions are expensive to compute
    \end{itemize}
    \item Hypertangent
    \begin{itemize}
        \item $\tanh(x)=2\sigma(2x)-1$: A zero-centered version of sigmoid
        \item Still, gradient vanishing problem and computation-expensive problem
    \end{itemize}
    \item Sigmoid, tanh are typically used at final layers
\end{itemize}
\begin{figures}
    \subfig{Sigmoid}{actfunc-sigmoid.jpg}{.18}
    \subfig{tanh}{actfunc-tanh.jpg}{.18}
\end{figures}

\subsubsection*{Linear Units}
\begin{equation}\begin{aligned}
    \text{ReLU}(x)=\max\{0,x\},&~\text{LeakyReLU}(x)=\max\{0.01 x,x\}\\
    \text{PReLU}(x)=\max\{\alpha x,x\}~(\alpha>0),&~\text{ELU}(x)=\begin{cases}
        x & (x>0) \\
        \alpha(e^x-1) & (x\leq 0)
    \end{cases}~(\alpha>0)
\end{aligned}\end{equation}
\begin{itemize}
    \item Pros of ReLU
    \begin{itemize}
        \item Gradient does not vanish for positive input; negative input is less important
        \item Easy to compute
        \item Model converges faster
    \end{itemize}
    \item Cons of ReLU
    \begin{itemize}
        \item Not zero-centered; Output always nonnegative
        \item Some neurons may become \textit{dead} (but, not often)
        \begin{itemize}
            \item Consider linear layer($W,b$) $\to$ ReLU ($f$)
            \item Consider backprop. on $i$-th neuron of linear layer, i.e. $\frac{\partial L}{\partial w_i}=\frac{\partial L}{\partial f}\frac{\partial f}{\partial w_i}$
            \item Remind: $\frac{\partial f}{\partial w_i}$ is $1$ if $x_j>0$, else $0$
            \item If $\forall x_j$, $w_i^Tx_j+b_i<0$, $\frac{\partial L}{\partial w_i}=0$, i.e. $w_i$ NEVER changes $\implies$ ``dead'' neuron
        \end{itemize}
    \end{itemize}
    \item Leaky ReLU: Most benefits of ReLU, Closer to zero-center, No saturation (dead neuron)
    \item Parametric ReLU(PReLU): introduce learnable param. $\alpha$
    \item Exponential Linear Units (ELU)
    \begin{itemize}
        \item Pros: Benefits of ReLU, Closer to zero-center, Robust to noise (saturation for $x\to-\infty$)
        \item Cons: Saturation issue for negative inputs
    \end{itemize}
    \item ReLU is mostly good choice, Leaky ReLU or ELU may be next good choice
\end{itemize}
\begin{figures}
    \subfig{ReLU}{actfunc-relu.jpg}{.18}
    \subfig{Leaky ReLU}{actfunc-leaky-relu.jpg}{.18}
    \subfig{ELU}{actfunc-elu.jpg}{.18}
\end{figures}

\subsection{Weights Initialization Methods}

\subsubsection*{Weights Must be Initialized Randomly}
\begin{itemize}
    \item Weights of linear layers must be initialized at first step of training
    \item If weights are initialized symmetrically?
    \begin{itemize}
        \item Consider linear layer $q=Wx$ $\to$ L2 loss layer $f$
        \begin{equation}
            L=f(q)=\norm{Wx}_2^2,~\nabla_Wf=2qx^T,~\nabla_xf=2W^Tq
        \end{equation}
        \item $[W]_{ij}=0$ $\implies$ $\nabla_Wf=0$ $\implies$ $W$ never changes
        \item $[W]_{ij}=1$ $\implies$ $\nabla_xf=2q$ $\implies$ Consistent gradients $\implies$ Limits directions on previous layer
    \end{itemize}
    \item We should provide \textit{various} linear scores over different regions
    \item $\therefore$ Weights must be initialized \textit{randomly}
\end{itemize}

\subsubsection*{Xavier-He Initialization}
\begin{itemize}
    \item Weights are initialized with large variance, e.g. $\mathcal{N}(0,1)$ $\implies$ Weight blows up, saturation
    \item Weights are initialized with small variance, e.g. $\mathcal{N}(0,0.1^2)$ $\implies$ Weight vanishes, no learning
    \item Xavier Initialization: Initialize weights with
    \begin{equation}
        \mathcal{N}\left(0,\frac{1}{n}\right)
    \end{equation}
    \begin{itemize}
        \item Output has same distribution as input
        \item $n$: Length of inner product(i.e. input size), convolution in the layer
        \item Initializing with $\mathcal{N}(0,1)$, then divide by $\sqrt{n}$ is identical way
    \end{itemize}
    \item He Initialization
    \begin{itemize}
        \item Pronounced [he]
        \item Half of outputs may be 0 for ReLU $\implies$ only half of weights are used
        \item Initialize with $\mathcal{N}(0,1)$, then divide by $\sqrt{n/2}$
    \end{itemize}
\end{itemize}
\clearpage
\begin{figures}
    \subfig{Initial}{weight-init-initial.png}{.12}
    \subfig{Blowed Up}{weight-init-big-var.png}{.12}
    \subfig{Vanished}{weight-init-small-var.png}{.12}
    \subfig{Consistent}{weight-init-proper-var.png}{.12}
\end{figures}

\subsection{Batch Normalization}

\subsubsection*{Data Preprocessing}
\begin{itemize}
    \item Data is too much off from zero $\implies$ saturation, especially sigmoid, tanh, etc
    \item Normalizing input data to zero-mean ($\mu=0$) and unit-variance ($\sigma^2=1$)
    \item Allows to use large l.r.s $\implies$ Fast training
    \item But, as data goes thru layers, mean and variance changes $\implies$ Normalize between layers
\end{itemize}

\subsubsection*{Batch Normalization of FC Layer}
\begin{itemize}
    \item Consider a \textit{batch} of input data $x$: $(N,D)$
    \begin{itemize}
        \item $N$: batch size, $D$: \# of channels
    \end{itemize}
    \item Batch Normalization (BatchNorm, BN)
    \begin{itemize}
        \item Mean and variance of $d$-th channel is
        \begin{equation}
            \mu_d=\frac{1}{N}\sum_{n=1}^Nx_{n,d},~\sigma_d^2=\frac{1}{N}\sum_{n=1}^N\left(x_{n,d}-\mu_d\right)^2
        \end{equation}
        \item Mean vector $\mu$, Stddev vector $\sigma$ is a vector of size $D$
        \item Normalized input is
        \begin{equation}
            \hat{x}_{n,d}=\frac{x_{n,d}-\mu_d}{\sqrt{\sigma_d^2+\varepsilon}}
        \end{equation}
        \item $\varepsilon>0$ prevents numerical instability
    \end{itemize}
    \item Batch Normalization with Learnable Parameters
    \begin{itemize}
        \item Strict zero-mean, unit-variance are not good $\implies$ give some shifts
        \begin{itemize}
            \item Not flexible (overfitting), activations may look too linear
        \end{itemize}
        \item Introduce learnable param.s $\gamma,\beta$ with size of $D$
        \item Output is
        \begin{equation}
            y_{n,d}=\gamma_d\hat{x}_{n,d}+\beta_d=\gamma_d\frac{x_{n,d}-\mu_d}{\sqrt{\sigma_d^2+\varepsilon}}+\beta_d
        \end{equation}
        \item $\gamma,\beta$ gets trained at training time, constant on testing time
    \end{itemize}
    \item BatchNorm for Conv. layers: $x:(N,D,H,W)\implies\mu,\sigma,\gamma,\beta:(1,D,1,1)$
    \item BN layers are usually inserted after FC/Conv. Layers, before activation
\end{itemize}

\subsubsection*{Other Normalizations}
\begin{itemize}
    \item BatchNorm is heavily affected by inputs within the batch
    \item Layer Normalization: Average/variance over channel dimension
    \begin{itemize}
        \item $x:(N,D)\implies\mu,\sigma:(N,1),~\gamma,\beta:(1,D)$
        \item Note BatchNorm: $x:(N,D)\implies\mu,\sigma,\gamma,\beta:(1,D)$
        \item Shows same behavior for training and test
        \item Used for RNN/Transformers
    \end{itemize}
    \item Instance Normalization: Average/variance over feature dimension for Conv. Layers
    \begin{itemize}
        \item $x:(N,D,W,H)\implies\mu,\sigma:(N,D,1,1),~\gamma,\beta:(1,D,1,1)$
        \item Note BatchNorm for Conv. Layer: $x:(N,D,H,W)\implies\mu,\sigma,\gamma,\beta:(1,D,1,1)$
        \item Shows same behavior for training and test
    \end{itemize}
\end{itemize}
\begin{figures}
    \fig{data-normalization.png}{.8}
\end{figures}

\subsection{Minor Tips for Training NN}

\subsubsection*{Data Augmentation}
\begin{itemize}
    \item Let the model to see \textit{diverse} version of data $\implies$ generalization
    \item Manipulate images in various methods and use them as inputs during training
    \item Popular methods
    \begin{itemize}
        \item Basic methods: flips, rotations, scales, crops, color jitter, etc
        \item Synthetic data: Entirely new, but fake data
        \item Random mix/combinations of basic methods
    \end{itemize}
    \item During test: average the prediction results among augmented inputs
\end{itemize}

\subsubsection*{Checking Model Correctness}
\begin{itemize}
    \item Check if the loss is reasonable, i.e. programmatic errors
    \item Regularization
    \begin{itemize}
        \item Suppose L2 regularization with data loss $L(W)+\lambda\norm{W}_2^2$
        \item Settings $\lambda=0$ should give smaller loss $\implies$ if not, loss has bug
    \end{itemize}
    \item Training loss must go down eventually
    \begin{itemize}
        \item Loss does not go down $\implies$ Maybe too low l.r.
        \item Loss blows up $\implies$ Maybe too high l.r.
    \end{itemize}
\end{itemize}
