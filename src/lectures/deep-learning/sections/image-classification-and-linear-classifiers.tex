\section{Image Classification and Linear Classifiers}

\subsection{Properties of Image Data}

\subsubsection*{Image Classification}
\begin{itemize}
    \item Mapping an image to a fixed set of category labels
    \item Challenges: No clear-cut way to classify an image definitely
    \begin{itemize}
        \item Semantic gap: Image는 실제 의미와는 다른 각 픽셀의 R,G,B 값으로 표현됨
        \item changes in viewpoint, illumination, deformation, occlusion, background clutter
    \end{itemize}
\end{itemize}
\begin{figures}
    \subfig{Original}{cat-original.jpg}{.1002}
    \subfig{Illumination}{cat-illumination.jpg}{.2209}
    \subfig{Deformation}{cat-deformation.jpg}{.1977}
    \subfig{Occlusion}{cat-occlusion.jpg}{.1835}
    \subfig{Background Clutter}{cat-background-clutter.jpg}{.1978}
\end{figures}

\subsection{Nearest Neighbor Classifiers}

\subsubsection*{Machine Learning Methods}
\begin{itemize}
    \item Data-driven approach
    \begin{enumerate}
        \item Collect: Dataset of images and their labels
        \item Train: Use ML to train a classifier
        \item Predict: Apply classifier on new images
    \end{enumerate}
\end{itemize}

\subsubsection*{Nearest Neighbor(NN) Classifier}
\begin{itemize}
    \item Train: Form database of all training images
    \item Predict
    \begin{enumerate}
        \item Calculate distances between a test image and all training images, respectively
        \begin{equation}\label{eq:dl:l1-distance}\begin{aligned}
            \text{L1: }d_1(I_1,I_2)&=\sum_{i\in\text{pixels}}|I_{1,i}-I_{2,i}|\\
            \text{L2: }d_1(I_1,I_2)&=\sqrt{\sum_{i\in\text{pixels}}(I_{1,i}-I_{2,i})^2}
        \end{aligned}\end{equation}
        \item Classify the test image as the class of the ``nearest'' (training) image
    \end{enumerate}
\end{itemize}

\subsubsection*{k-Nearest Neighbor(kNN) Classifier}
\begin{itemize}
    \item Train: Form database of all training images
    \item Predict
    \begin{enumerate}
        \item Calculate distances(\ref{eq:dl:l1-distance}) between a test image and all training images, respectively
        \item Select classes of $k$ nearest images
        \item Classify the test image as mode value of the selected labels
    \end{enumerate}
\end{itemize}

\subsubsection*{Setting Hyperparameters}
\begin{itemize}
    \item \# of classes to select($k$), Distance(norm) to use $\Rightarrow$ Hyperparameters
    \begin{itemize}
        \item Mostly, hparams(hyperparameters) are problem-dependent
        \item Design choices, not directly learned from training data $\Rightarrow$ train-and-error
    \end{itemize}
    \item Idea \#1: Choose hparams that works best on all data
    \begin{itemize}
        \item Only learns training data $\Rightarrow$ Overfitting
        \item $k=1$ will always the best
    \end{itemize}
    \item Idea \#2
    \begin{itemize}
        \item Method
        \begin{enumerate}
            \item dataset $\Rightarrow$ train + test
            \item Adjust hparams until the model works best for test dataset
        \end{enumerate}
        \item Cannot verify the model works well for unseen data
        \item Test data SHOULD NOT be used for training
    \end{itemize}
    \item Idea \#3
    \begin{itemize}
        \item Method
        \begin{enumerate}
            \item dataset $\Rightarrow$ train + validation + test
            \item Adjust hparams until the model works best for validation dataset
            \item Verify model on test dataset
        \end{enumerate}
        \item Often used
    \end{itemize}
    \item Idea \#4: $k$-fold cross-validation
    \begin{itemize}
        \item Method
        \begin{enumerate}
            \item dataset $\Rightarrow$ (train+validation) + test
            \item Partition train+validation set into $k$ folds
            \item Select 1 fold as validation set, remainder as train set and adjust hparams
            \item Repeat adjusting hparams by alternating folds
            \item Set hparams by averaging the results
            \item Verify on test dataset
        \end{enumerate}
        \item Useful for small dataset
        \item Not often used
    \end{itemize}
\end{itemize}

\subsubsection*{Pros and Cons of kNN Classifier}
\begin{itemize}
    \item Pros: Training is fast - Just store all training images
    \item Cons
    \begin{itemize}
        \item Translation: Small translation causes very far distance
        \item Slow: Prediction is slow: Goes through ALL training images, $O(n)$
        \item Curse of dimensionality: Small increase of dimension causes tremendous increase of calculation
    \end{itemize}
    \item kNN Classifier is a BAD image classifier
\end{itemize}
\clearpage

\subsection{Linear Classifiers}

\subsubsection*{Parametric Approach}
\begin{itemize}
    \item Assume \# of classes is $m$
    \item Assume image has $n$ data $\Rightarrow$ Stretch to $n\times 1$ vector $x$
    \begin{itemize}
        \item $32\times 32$ RGB image has $32\times 32\times 3=3072$ data
    \end{itemize}
    \item Matrix of parameters $W$ is an $m\times n$ matrix
    \item Vector of bias $b$ is an $n\times 1$ vector
    \item Score of $x$ for each class $s$ can be obtained in a linear manner
    \begin{equation}
        s=f(x,W)=Wx+b
    \end{equation}
    \item Reshaping each row of $W$ into shape of image generates ``templates'' $\Rightarrow$ Template matching
\end{itemize}

\subsubsection*{Importance}
\begin{itemize}
    \item Linear classifier itself is not a good image classifier
    \item ``Basic blocks'' for complex classifier
    \begin{itemize}
        \item Multilayer neural network repeats linear/nonlinear classifier
    \end{itemize}
    \item Relatively large scores are unnecessary
\end{itemize}

\subsection{SVM-loss Classifier}

\subsubsection*{Linear Classifiers}
\begin{itemize}
    \item Linear binary classifier
    \begin{itemize}
        \item Separating points into two sets: $X=\{x_1,x_2,\cdots,x_N\}$ and $Y=\{y_1,y_2,\cdots,y_M\}$
        \item 경계에 가장 가까운 $X$, $Y$의 점: $\hat{x}$, $\hat{y}$ $\Rightarrow$ $\exists~w,b$ s.t. $w^{\top}\hat{x}+b=1$, $w^{\top}\hat{y}+b=-1$
        \item $\forall~x\in X$, $w^{\top}x+b\geq1$, $\forall~y\in Y$, $w^{\top}y+b\leq-1$
        \item Boundary hyperplane $w^{\top}x+b=0$: $\hat{x}$, $\hat{y}$까지의 거리가 같음
    \end{itemize}
    \item Robust linear classifier
    \begin{itemize}
        \item In case the points can be separated into two sets completely
        \item Let $\mathcal{H}_1=\{z~|~w^{\top}z+b=1\}$, $\mathcal{H}_2=\{z~|~w^{\top}z+b=-1\}$
        \item Maximize the margin between two sets: $\mathrm{dist}(\mathcal{H}_1,\mathcal{H}_2)=\frac{2}{\Vert w\Vert_2}$
        \begin{equation}\begin{aligned}
            \mathrm{minimize}~~&~~\Vert w\Vert_2 \\
            \mathrm{subject~to}~~&~~w^{\top}x_i+b\geq1\\
                &~~w^{\top}y_i+b\leq-1\\
                &~~(i=1,2,\cdots,M)
        \end{aligned}\end{equation}
    \end{itemize}
    \clearpage
    \item Approximate linear classifier
    \begin{itemize}
        \item In case the points cannot be separated into two sets completely $\Rightarrow$ Allow some violations
        \item Objective: minimizing \# of missclassified points
        \begin{equation}\begin{aligned}
            \mathrm{minimize}~~&~~\sum_{i}u_i \\
            \mathrm{subject~to}~~&~~w^{\top}x_i+b\geq1-u_i~~(i=1,2,\cdots,N) \\
                &~~w^{\top}y_i+b\leq-1+u_i~~(i=1,2,\cdots,M) \\
                &~~u_i\geq 0
        \end{aligned}\end{equation}
        \item $u_i$: \# of missclassified points; slack variable
        \item Larger $u_i$ allows more violations
    \end{itemize}
    \item Support Vector Machine (SVM)
    \begin{itemize}
        \item Tradeoff between robust L.C. and approximate L.C.
        \item Introduces a hyperparameter $\gamma>0$
        \item Large $\gamma$ $\Rightarrow$ Less slack; Small $\gamma$ $\Rightarrow$ Less separate distance
        \begin{equation}\begin{aligned}
            \mathrm{minimize}~~&~~\Vert w\Vert_2^2+\gamma\sum_{i}u_i \\
            \mathrm{subject~to}~~&~~w^{\top}x_i+b\geq1-u_i~~(i=1,2,\cdots,N) \\
                &~~w^{\top}y_i+b\leq-1+u_i~~(i=1,2,\cdots,M) \\
                &~~u_i\geq0
        \end{aligned}\end{equation}
    \end{itemize}
\end{itemize}
\begin{figures}
    \subfig{Robust L.C.}{robust-linear-classifier.PNG}{.309}
    \subfig{Approximate L.C.}{approximate-linear-classifier.PNG}{.296}
    \subfig{Support Vector Machine}{support-vector-machine.PNG}{.295}
\end{figures}

\subsubsection*{Multiclass SVM}
\begin{itemize}
    \item Form SVM for each class $k=1,2,\cdots,K$ $\Rightarrow$ $K$ binary classifiers
    \begin{equation}\begin{aligned}
        \mathrm{minimize}~~&~~\Vert w_k\Vert_2^2+\gamma\sum_{i}u_i \\
        \mathrm{subject~to}~~&~~w_k^{\top}x_i+b_k\geq1-u_i~~(k=y_i) \\
            &~~w_k^{\top}y_i+b_k\leq-1+u_i~~(k\neq y_i) \\
            &~~u_i\geq0,~\forall i
    \end{aligned}\end{equation}
    \begin{itemize}
        \item $y_i$: ground-truth label of input $x_i$
    \end{itemize}
    \item One-vs-rest classifier
    \begin{enumerate}
        \item Solve $K$ optimization problems
        \item $\forall k$, compute $s_k=w_k^{\top}x+b_k$
        \item Select class $k^\ast$ with maximum $s_{k^\ast}$
    \end{enumerate}
    \begin{figures}
        \fig{multiclass-svm.png}{.3}
    \end{figures}
\end{itemize}

\subsubsection*{Multiclass SVM Loss Function}
\begin{itemize}
    \item $K$ SVMs can be combined into one big SVM
    \begin{equation}\begin{aligned}
        \mathrm{minimize}~~&~~\sum_k\Vert w_k\Vert_2^2+\gamma\sum_{i}\sum_{k\neq y_i}u_{i,k} \\
        \mathrm{subject~to}~~&~~s_{y_i}\geq s_{k}+b_k+2~~(k\neq y_i)\\
            &~~s_j=w_j^{\top}x_i+b_j\\
            &~~u_{i,k}\geq 0,~\forall i,~k
    \end{aligned}\end{equation}
    \begin{itemize}
        \item $y_i$: ground-truth label of $x_i$
        \item $s_j=w_j^{\top}x_i+b_i$ is a linear score for label $j$
    \end{itemize}
    \item At optimality, slack value of $i$-th input classified as label $j$ is $u_{i,j}=\max\{1+s_j-s_{y_i},0\}$ ($s_j=w_j^{\top}x_i+b_j$)
    \item Loss function for $i$-th input whose ground-truth label is $y_i$ is
    \begin{equation}
        L_i=\sum_{j\neq y_i}\max\{0,s_j-s_{y_i}+1\}
    \end{equation}
    \begin{itemize}
        \item Also called ``hinge loss''
    \end{itemize}
    \begin{figures}
        \fig{hinge-loss.png}{.4}
    \end{figures}
    \item Average loss: $L(W)=\frac{1}{N}\sum_{i=1}^NL_i$
    \item Term $\sum_k\Vert w_k\Vert_2^2$ makes SVM consider regulariation as well
\end{itemize}

\subsection{Softmax Classifier}

\subsubsection*{Softmax Classifier}
\begin{itemize}
    \item On binary classifier using logistic regression, we modelled $\log\frac{\mathbb{P}(a_i|b=0)}{\mathbb{P}(a_i|b=1)}=-a_ix_i$
    \item Similarly, define log-ratio of conditional probability as hyperplane decision boundary
    \begin{equation}
        \log\frac{\mathbb{P}(Y=j|X)}{\mathbb{P}(Y=y_i|X)}=\begin{cases}
            w_j^{\top}x_i+b_j & (j\neq y_i)\\
            0 & (j=y_i)
        \end{cases}
    \end{equation}
    \item Solving these equations with $\sum_{k=1}^K\mathbb{P}(Y=k)=1$:
    \begin{itemize}
        \item $\mathbb{P}(Y=j|X)=\frac{\exp(w_j^{\top}x_i+b_j)}{1+\sum_{k\neq y_i}\exp(w_k^{\top}x_i+b_k)}$ ($j\neq y_k$)
        \item $\mathbb{P}(Y=y_i|X)=\frac{1}{1+\sum_{k\neq y_i}\exp(w_k^{\top}x_i+b_k)}$
    \end{itemize}
    \item Alternatively, we get softmax of linear scores
    \begin{equation}
        P(Y=j)=\frac{\exp(w_j^{\top}x_i+b_j)}{\sum_{k=1}^K\exp(w_k^{\top}x_i+b_k)}=\text{softmax}(Wx_i+b)
    \end{equation}
\end{itemize}

\subsubsection*{Loss Function}
\begin{itemize}
    \item $E_i=[e_i]_K$: one-hot encoding s.t. $e_{y_i}=1$, else is $0$
    \item Loss function is cross-entropy of $E_i$ and softmax of linear scores
    \begin{equation}
        L_i=-E_i^{\top}\log\Bigr[\text{softmax}(Wx_i+b)\Bigr]=-\log\left(\frac{e^{s_{y_i}}}{\sum_j e^{s_j}}\right),~s_k=w_kx_i+b_k
    \end{equation}
    \begin{itemize}
        \item Cross-entropy $-p^{\top}\log q$ is a discrepancy caused by using dist. $q$ instead of true dist. $p$
    \end{itemize}
    \item Average loss: $L(W)=\frac{1}{N}\sum_{i=1}^NL_i$
\end{itemize}
